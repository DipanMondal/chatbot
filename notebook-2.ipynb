{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876080a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "import re\n",
    "from tensorflow import keras\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "\n",
    "dir_path = r'C:\\Users\\idipa\\PycharmProject\\ChatBot\\ChatbotData'\n",
    "files_list = os.listdir(dir_path + os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e5221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = [], []\n",
    "\n",
    "for filepath in files_list:\n",
    "    file_ = open(dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(file_)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len(con) > 2 :\n",
    "            replies = con[1 :]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                questions.append(con[0])\n",
    "                answers.append(rep)\n",
    "        elif len(con)> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061a4677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.',\n",
       " 'AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.',\n",
       " 'Sort of.',\n",
       " \"By the strictest dictionary definition of the word 'sentience', I may be.\",\n",
       " \"Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be.\",\n",
       " \"In all probability, I am not.  I'm not that sophisticated.\",\n",
       " 'Do you think I am?',\n",
       " 'How would you feel about me if I told you I was?',\n",
       " 'No.',\n",
       " 'Python.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac6775e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is AI?',\n",
       " 'What is AI?',\n",
       " 'Are you sentient?',\n",
       " 'Are you sentient?',\n",
       " 'Are you sentient?',\n",
       " 'Are you sapient?',\n",
       " 'Are you sapient?',\n",
       " 'Are you sapient?',\n",
       " 'Are you sapient?',\n",
       " 'What language are you written in?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aea6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_with_tags = []\n",
    "for i in range(len(answers)):\n",
    "    if type(answers[i]) == str:\n",
    "        answers_with_tags.append(answers[i])\n",
    "    else:\n",
    "        questions.pop(i)\n",
    "\n",
    "answers = []\n",
    "for i in range(len(answers_with_tags)) :\n",
    "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9d04a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think. <END>',\n",
       " '<START> AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind. <END>',\n",
       " '<START> Sort of. <END>',\n",
       " \"<START> By the strictest dictionary definition of the word 'sentience', I may be. <END>\",\n",
       " \"<START> Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be. <END>\",\n",
       " \"<START> In all probability, I am not.  I'm not that sophisticated. <END>\",\n",
       " '<START> Do you think I am? <END>',\n",
       " '<START> How would you feel about me if I told you I was? <END>',\n",
       " '<START> No. <END>',\n",
       " '<START> Python. <END>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b77847",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall\",\n",
    "    \"he'll've\": \"he shall have\",\n",
    "    \"he's\": \"he has\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has\",\n",
    "    \"i'd\": \"i had\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i shall\",\n",
    "    \"i'll've\": \"i shall have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall\",\n",
    "    \"it'll've\": \"it shall have\",\n",
    "    \"it's\": \"it has\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall\",\n",
    "    \"she'll've\": \"she shall have\",\n",
    "    \"she's\": \"she has\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall\",\n",
    "    \"they'll've\": \"they shall have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall\",\n",
    "    \"what'll've\": \"what shall have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who has\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'alls\": \"you alls\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall\",\n",
    "    \"you'll've\": \"you shall have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0af90747",
   "metadata": {},
   "outputs": [],
   "source": [
    "jo = json.dumps(contractions_dict)\n",
    "with open('contractions.json','w') as file:\n",
    "    file.write(jo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e139402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe it has already 2024! You have got to be kidding me.\n"
     ]
    }
   ],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(re.escape(key) for key in contractions_dict.keys()), re.IGNORECASE)\n",
    "\n",
    "def expand_contractions(sentence, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        # Match is case-insensitive, use the original case in replacement\n",
    "        contraction = match.group(0)\n",
    "        expanded = contractions_dict.get(contraction.lower())\n",
    "        if contraction[0].isupper():\n",
    "            expanded = expanded.capitalize()\n",
    "        return expanded\n",
    "    return contractions_re.sub(replace, sentence)\n",
    "\n",
    "# Example usage\n",
    "sentence = \"I can't believe it's already 2024! You've got to be kidding me.\"\n",
    "expanded_sentence = expand_contractions(sentence)\n",
    "print(expanded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08e284b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> Yes I am inspired by commander Data ' s artificial personality .  <END>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " re.sub(r\"\"\"([+$@#%^&.?!*\"\\\\',:;-])\"\"\", r' \\1 ', answers[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d65e1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(answers)):\n",
    "    st = expand_contractions(answers[i].lower())\n",
    "    answers[i] = re.sub(r\"\"\"([+$@#%^&.?!*\"\\\\',:;-])\"\"\", r' \\1 ', st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c27b15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> artificial intelligence is the branch of engineering and science devoted to constructing machines that think .  <end>',\n",
       " '<start> ai is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind .  <end>',\n",
       " '<start> sort of .  <end>',\n",
       " \"<start> by the strictest dictionary definition of the word  ' sentience '  ,  i may be .  <end>\",\n",
       " '<start> even though i am a construct i do have a subjective experience of the universe ,  as simplistic as it may be .  <end>',\n",
       " '<start> in all probability ,  i am not .   i am not that sophisticated .  <end>',\n",
       " '<start> do you think i am ?  <end>',\n",
       " '<start> how would you feel about me if i told you i was ?  <end>',\n",
       " '<start> no .  <end>',\n",
       " '<start> python .  <end>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbe49d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " 'by',\n",
       " 'the',\n",
       " 'strictest',\n",
       " 'dictionary',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'word',\n",
       " \"'\",\n",
       " 'sentience',\n",
       " \"'\",\n",
       " ',',\n",
       " 'i',\n",
       " 'may',\n",
       " 'be',\n",
       " '.',\n",
       " '<end>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[3].strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6225b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(questions)):\n",
    "    st =  expand_contractions(questions[i].lower())\n",
    "    questions[i] = re.sub(r\"\"\"([+$@#%^&.?!*\"\\\\',:;-])\"\"\", r' \\1 ', st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd1d39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "Once upon a time, in a quaint little village nestled in the verdant hills, there lived an eclectic group of people, each with unique stories and backgrounds. The village, known as Greenfield, was renowned for its picturesque landscapes, vibrant community life, and rich cultural heritage. Among the residents was Alice, an astute librarian with an insatiable curiosity about the world. Her house was a haven for books, maps, and artifacts from different eras and regions, reflecting her lifelong passion for knowledge and adventure.\n",
    "\n",
    "Alice often spent her days in the village library, a grand building with towering shelves filled with volumes of literature, science, history, and art. The library was a hub of activity, attracting scholars, students, and readers from all walks of life. One day, as she was cataloging a collection of ancient manuscripts, she discovered a dusty old tome that seemed out of place. The book, bound in weathered leather, was inscribed with symbols and languages she had never seen before.\n",
    "\n",
    "Intrigued, Alice began to decipher its contents, which narrated the tales of an ancient civilization known for its wisdom and technological advancements. The manuscript spoke of a lost city, hidden deep within an uncharted jungle, protected by intricate puzzles and mythical creatures. The allure of uncovering such a mystery captivated Alice, and she decided to embark on a quest to find this lost city.\n",
    "\n",
    "She shared her discovery with her close friends, each bringing their own set of skills to the journey. There was Marcus, a seasoned archaeologist with a knack for solving riddles; Elena, a brilliant linguist fluent in multiple languages; and Leo, an intrepid explorer with unmatched survival skills. Together, they formed a formidable team, ready to face the unknown.\n",
    "\n",
    "Their journey began with meticulous planning, gathering supplies, and studying maps and historical texts. They traveled across continents, through bustling cities and remote villages, encountering diverse cultures and landscapes along the way. Their path led them through dense forests, arid deserts, and treacherous mountains, each step bringing them closer to their goal.\n",
    "\n",
    "As they ventured deeper into the jungle, they faced numerous challenges. The thick canopy overhead blocked the sunlight, making navigation difficult. They encountered wild animals, torrential rains, and steep cliffs that tested their endurance and resilience. Despite the hardships, their determination never wavered.\n",
    "\n",
    "One fateful day, they stumbled upon an ancient stone path, overgrown with vines and moss. The path led to a massive stone gate, adorned with intricate carvings depicting scenes of a thriving civilization. The gate was guarded by a colossal statue of a mythical beast, its eyes seemingly watching their every move.\n",
    "\n",
    "Using their combined knowledge, the team deciphered the carvings, revealing clues to unlock the gate. After hours of meticulous work, they succeeded, and the gate slowly creaked open, revealing the entrance to the lost city. The sight that greeted them was beyond their wildest dreams: towering structures, ornate temples, and lush gardens, all remarkably preserved despite the passage of time.\n",
    "\n",
    "As they explored the city, they uncovered advanced technologies and sophisticated art, evidence of a highly developed society. They also found records of the city's history, detailing its rise and fall. The city had once been a beacon of knowledge and innovation, but a cataclysmic event had forced its inhabitants to abandon it, leaving behind their legacy for future generations to discover.\n",
    "\n",
    "Throughout their exploration, the team encountered various puzzles and traps, designed to protect the city's secrets. Each challenge required a blend of intellect, teamwork, and courage to overcome. They faced rooms that shifted like labyrinths, mechanisms that required precise timing, and guardians that tested their resolve.\n",
    "\n",
    "Among the most remarkable discoveries was a vast library, containing scrolls and tablets that held the collective wisdom of the ancient civilization. Alice and Elena were particularly enthralled by the linguistic and historical treasures they found, while Marcus and Leo marveled at the architectural and engineering feats.\n",
    "\n",
    "Their greatest challenge came when they discovered a hidden chamber, protected by a series of complex locks and puzzles. The chamber was said to hold the most valuable artifact of the lost civilization, a relic of immense power and knowledge. Solving the final puzzle required all their skills and collaboration, but eventually, they succeeded.\n",
    "\n",
    "Inside the chamber, they found a crystalline artifact, glowing with an ethereal light. As they carefully examined it, they realized it contained vast amounts of data, encoded in a way that was far beyond their current understanding. The artifact held the key to unlocking further mysteries of the lost civilization and potentially advancing modern technology and knowledge.\n",
    "\n",
    "Their discovery marked a significant milestone in the field of archaeology and history. The lost city, once a myth, had become a reality, offering insights into a civilization that was both advanced and enigmatic. The team's findings were documented and shared with the world, leading to new research and explorations.\n",
    "\n",
    "Alice, Marcus, Elena, and Leo returned to Greenfield as heroes, their adventure becoming the stuff of legends. They will continue their work, inspired by their journey and the knowledge they had gained. Their story will serve as a reminder of the endless possibilities that await those who dare to explore the unknown.\n",
    "\n",
    "In Greenfield, life continued to thrive, with the community drawing inspiration from the team's achievements. The village became a center for learning and exploration, attracting scholars and adventurers from far and wide. The library, once a quiet haven, buzzed with activity as people sought to learn more about the lost civilization and its secrets.\n",
    "\n",
    "The team's legacy will live on, inspiring future generations to pursue their dreams and explore the mysteries of the world. Alice will continue her work at the library, always on the lookout for the next great adventure. Marcus will return to his archaeological pursuits, uncovering more hidden treasures and ancient sites. Elena will dedicate herself to deciphering the languages and texts of the lost civilization, while Leo will embark on new expeditions, driven by his insatiable curiosity.\n",
    "\n",
    "Their story will become a testament to the power of curiosity, collaboration, and perseverance. It will show that with determination and a willingness to face the unknown, even the most elusive mysteries can be uncovered. The lost city, once hidden in the depths of the jungle, had revealed its secrets, thanks to the unwavering spirit of those who dared to seek it.\n",
    "\n",
    "And so, the tale of Greenfield and its intrepid explorers will continue, a shining example of what can be achieved when people come together with a shared vision and a relentless pursuit of knowledge. Their adventure will have only just begun, with the promise of more discoveries and stories waiting to be told.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8dedfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOnce upon a time, in a quaint little village nestled in the verdant hills, there lived an eclectic group of people, each with unique stories and backgrounds. The village, known as Greenfield, was renowned for its picturesque landscapes, vibrant community life, and rich cultural heritage. Among the residents was Alice, an astute librarian with an insatiable curiosity about the world. Her house was a haven for books, maps, and artifacts from different eras and regions, reflecting her lifelong passion for knowledge and adventure.\\n\\nAlice often spent her days in the village library, a grand building with towering shelves filled with volumes of literature, science, history, and art. The library was a hub of activity, attracting scholars, students, and readers from all walks of life. One day, as she was cataloging a collection of ancient manuscripts, she discovered a dusty old tome that seemed out of place. The book, bound in weathered leather, was inscribed with symbols and languages she had never seen before.\\n\\nIntrigued, Alice began to decipher its contents, which narrated the tales of an ancient civilization known for its wisdom and technological advancements. The manuscript spoke of a lost city, hidden deep within an uncharted jungle, protected by intricate puzzles and mythical creatures. The allure of uncovering such a mystery captivated Alice, and she decided to embark on a quest to find this lost city.\\n\\nShe shared her discovery with her close friends, each bringing their own set of skills to the journey. There was Marcus, a seasoned archaeologist with a knack for solving riddles; Elena, a brilliant linguist fluent in multiple languages; and Leo, an intrepid explorer with unmatched survival skills. Together, they formed a formidable team, ready to face the unknown.\\n\\nTheir journey began with meticulous planning, gathering supplies, and studying maps and historical texts. They traveled across continents, through bustling cities and remote villages, encountering diverse cultures and landscapes along the way. Their path led them through dense forests, arid deserts, and treacherous mountains, each step bringing them closer to their goal.\\n\\nAs they ventured deeper into the jungle, they faced numerous challenges. The thick canopy overhead blocked the sunlight, making navigation difficult. They encountered wild animals, torrential rains, and steep cliffs that tested their endurance and resilience. Despite the hardships, their determination never wavered.\\n\\nOne fateful day, they stumbled upon an ancient stone path, overgrown with vines and moss. The path led to a massive stone gate, adorned with intricate carvings depicting scenes of a thriving civilization. The gate was guarded by a colossal statue of a mythical beast, its eyes seemingly watching their every move.\\n\\nUsing their combined knowledge, the team deciphered the carvings, revealing clues to unlock the gate. After hours of meticulous work, they succeeded, and the gate slowly creaked open, revealing the entrance to the lost city. The sight that greeted them was beyond their wildest dreams: towering structures, ornate temples, and lush gardens, all remarkably preserved despite the passage of time.\\n\\nAs they explored the city, they uncovered advanced technologies and sophisticated art, evidence of a highly developed society. They also found records of the city's history, detailing its rise and fall. The city had once been a beacon of knowledge and innovation, but a cataclysmic event had forced its inhabitants to abandon it, leaving behind their legacy for future generations to discover.\\n\\nThroughout their exploration, the team encountered various puzzles and traps, designed to protect the city's secrets. Each challenge required a blend of intellect, teamwork, and courage to overcome. They faced rooms that shifted like labyrinths, mechanisms that required precise timing, and guardians that tested their resolve.\\n\\nAmong the most remarkable discoveries was a vast library, containing scrolls and tablets that held the collective wisdom of the ancient civilization. Alice and Elena were particularly enthralled by the linguistic and historical treasures they found, while Marcus and Leo marveled at the architectural and engineering feats.\\n\\nTheir greatest challenge came when they discovered a hidden chamber, protected by a series of complex locks and puzzles. The chamber was said to hold the most valuable artifact of the lost civilization, a relic of immense power and knowledge. Solving the final puzzle required all their skills and collaboration, but eventually, they succeeded.\\n\\nInside the chamber, they found a crystalline artifact, glowing with an ethereal light. As they carefully examined it, they realized it contained vast amounts of data, encoded in a way that was far beyond their current understanding. The artifact held the key to unlocking further mysteries of the lost civilization and potentially advancing modern technology and knowledge.\\n\\nTheir discovery marked a significant milestone in the field of archaeology and history. The lost city, once a myth, had become a reality, offering insights into a civilization that was both advanced and enigmatic. The team's findings were documented and shared with the world, leading to new research and explorations.\\n\\nAlice, Marcus, Elena, and Leo returned to Greenfield as heroes, their adventure becoming the stuff of legends. They will continue their work, inspired by their journey and the knowledge they had gained. Their story will serve as a reminder of the endless possibilities that await those who dare to explore the unknown.\\n\\nIn Greenfield, life continued to thrive, with the community drawing inspiration from the team's achievements. The village became a center for learning and exploration, attracting scholars and adventurers from far and wide. The library, once a quiet haven, buzzed with activity as people sought to learn more about the lost civilization and its secrets.\\n\\nThe team's legacy will live on, inspiring future generations to pursue their dreams and explore the mysteries of the world. Alice will continue her work at the library, always on the lookout for the next great adventure. Marcus will return to his archaeological pursuits, uncovering more hidden treasures and ancient sites. Elena will dedicate herself to deciphering the languages and texts of the lost civilization, while Leo will embark on new expeditions, driven by his insatiable curiosity.\\n\\nTheir story will become a testament to the power of curiosity, collaboration, and perseverance. It will show that with determination and a willingness to face the unknown, even the most elusive mysteries can be uncovered. The lost city, once hidden in the depths of the jungle, had revealed its secrets, thanks to the unwavering spirit of those who dared to seek it.\\n\\nAnd so, the tale of Greenfield and its intrepid explorers will continue, a shining example of what can be achieved when people come together with a shared vision and a relentless pursuit of knowledge. Their adventure will have only just begun, with the promise of more discoveries and stories waiting to be told.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = expand_contractions(story)\n",
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d21727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nonce upon a time ,  in a quaint little village nestled in the verdant hills ,  there lived an eclectic group of people ,  each with unique stories and backgrounds .  the village ,  known as greenfield ,  was renowned for its picturesque landscapes ,  vibrant community life ,  and rich cultural heritage .  among the residents was alice ,  an astute librarian with an insatiable curiosity about the world .  her house was a haven for books ,  maps ,  and artifacts from different eras and regions ,  reflecting her lifelong passion for knowledge and adventure . \\n\\nalice often spent her days in the village library ,  a grand building with towering shelves filled with volumes of literature ,  science ,  history ,  and art .  the library was a hub of activity ,  attracting scholars ,  students ,  and readers from all walks of life .  one day ,  as she was cataloging a collection of ancient manuscripts ,  she discovered a dusty old tome that seemed out of place .  the book ,  bound in weathered leather ,  was inscribed with symbols and languages she had never seen before . \\n\\nintrigued ,  alice began to decipher its contents ,  which narrated the tales of an ancient civilization known for its wisdom and technological advancements .  the manuscript spoke of a lost city ,  hidden deep within an uncharted jungle ,  protected by intricate puzzles and mythical creatures .  the allure of uncovering such a mystery captivated alice ,  and she decided to embark on a quest to find this lost city . \\n\\nshe shared her discovery with her close friends ,  each bringing their own set of skills to the journey .  there was marcus ,  a seasoned archaeologist with a knack for solving riddles ;  elena ,  a brilliant linguist fluent in multiple languages ;  and leo ,  an intrepid explorer with unmatched survival skills .  together ,  they formed a formidable team ,  ready to face the unknown . \\n\\ntheir journey began with meticulous planning ,  gathering supplies ,  and studying maps and historical texts .  they traveled across continents ,  through bustling cities and remote villages ,  encountering diverse cultures and landscapes along the way .  their path led them through dense forests ,  arid deserts ,  and treacherous mountains ,  each step bringing them closer to their goal . \\n\\nas they ventured deeper into the jungle ,  they faced numerous challenges .  the thick canopy overhead blocked the sunlight ,  making navigation difficult .  they encountered wild animals ,  torrential rains ,  and steep cliffs that tested their endurance and resilience .  despite the hardships ,  their determination never wavered . \\n\\none fateful day ,  they stumbled upon an ancient stone path ,  overgrown with vines and moss .  the path led to a massive stone gate ,  adorned with intricate carvings depicting scenes of a thriving civilization .  the gate was guarded by a colossal statue of a mythical beast ,  its eyes seemingly watching their every move . \\n\\nusing their combined knowledge ,  the team deciphered the carvings ,  revealing clues to unlock the gate .  after hours of meticulous work ,  they succeeded ,  and the gate slowly creaked open ,  revealing the entrance to the lost city .  the sight that greeted them was beyond their wildest dreams :  towering structures ,  ornate temples ,  and lush gardens ,  all remarkably preserved despite the passage of time . \\n\\nas they explored the city ,  they uncovered advanced technologies and sophisticated art ,  evidence of a highly developed society .  they also found records of the city ' s history ,  detailing its rise and fall .  the city had once been a beacon of knowledge and innovation ,  but a cataclysmic event had forced its inhabitants to abandon it ,  leaving behind their legacy for future generations to discover . \\n\\nthroughout their exploration ,  the team encountered various puzzles and traps ,  designed to protect the city ' s secrets .  each challenge required a blend of intellect ,  teamwork ,  and courage to overcome .  they faced rooms that shifted like labyrinths ,  mechanisms that required precise timing ,  and guardians that tested their resolve . \\n\\namong the most remarkable discoveries was a vast library ,  containing scrolls and tablets that held the collective wisdom of the ancient civilization .  alice and elena were particularly enthralled by the linguistic and historical treasures they found ,  while marcus and leo marveled at the architectural and engineering feats . \\n\\ntheir greatest challenge came when they discovered a hidden chamber ,  protected by a series of complex locks and puzzles .  the chamber was said to hold the most valuable artifact of the lost civilization ,  a relic of immense power and knowledge .  solving the final puzzle required all their skills and collaboration ,  but eventually ,  they succeeded . \\n\\ninside the chamber ,  they found a crystalline artifact ,  glowing with an ethereal light .  as they carefully examined it ,  they realized it contained vast amounts of data ,  encoded in a way that was far beyond their current understanding .  the artifact held the key to unlocking further mysteries of the lost civilization and potentially advancing modern technology and knowledge . \\n\\ntheir discovery marked a significant milestone in the field of archaeology and history .  the lost city ,  once a myth ,  had become a reality ,  offering insights into a civilization that was both advanced and enigmatic .  the team ' s findings were documented and shared with the world ,  leading to new research and explorations . \\n\\nalice ,  marcus ,  elena ,  and leo returned to greenfield as heroes ,  their adventure becoming the stuff of legends .  they will continue their work ,  inspired by their journey and the knowledge they had gained .  their story will serve as a reminder of the endless possibilities that await those who dare to explore the unknown . \\n\\nin greenfield ,  life continued to thrive ,  with the community drawing inspiration from the team ' s achievements .  the village became a center for learning and exploration ,  attracting scholars and adventurers from far and wide .  the library ,  once a quiet haven ,  buzzed with activity as people sought to learn more about the lost civilization and its secrets . \\n\\nthe team ' s legacy will live on ,  inspiring future generations to pursue their dreams and explore the mysteries of the world .  alice will continue her work at the library ,  always on the lookout for the next great adventure .  marcus will return to his archaeological pursuits ,  uncovering more hidden treasures and ancient sites .  elena will dedicate herself to deciphering the languages and texts of the lost civilization ,  while leo will embark on new expeditions ,  driven by his insatiable curiosity . \\n\\ntheir story will become a testament to the power of curiosity ,  collaboration ,  and perseverance .  it will show that with determination and a willingness to face the unknown ,  even the most elusive mysteries can be uncovered .  the lost city ,  once hidden in the depths of the jungle ,  had revealed its secrets ,  thanks to the unwavering spirit of those who dared to seek it . \\n\\nand so ,  the tale of greenfield and its intrepid explorers will continue ,  a shining example of what can be achieved when people come together with a shared vision and a relentless pursuit of knowledge .  their adventure will have only just begun ,  with the promise of more discoveries and stories waiting to be told . \\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = re.sub(r\"\"\"([+$@#%^&.?!*\"\\\\',:;-])\"\"\", r' \\1 ', story.lower())\n",
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0404f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['once',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'time',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'quaint',\n",
       " 'little',\n",
       " 'village',\n",
       " 'nestled',\n",
       " 'in',\n",
       " 'the',\n",
       " 'verdant',\n",
       " 'hills',\n",
       " ',',\n",
       " 'there',\n",
       " 'lived',\n",
       " 'an',\n",
       " 'eclectic',\n",
       " 'group',\n",
       " 'of',\n",
       " 'people',\n",
       " ',',\n",
       " 'each',\n",
       " 'with',\n",
       " 'unique',\n",
       " 'stories',\n",
       " 'and',\n",
       " 'backgrounds',\n",
       " '.',\n",
       " 'the',\n",
       " 'village',\n",
       " ',',\n",
       " 'known',\n",
       " 'as',\n",
       " 'greenfield',\n",
       " ',',\n",
       " 'was',\n",
       " 'renowned',\n",
       " 'for',\n",
       " 'its',\n",
       " 'picturesque',\n",
       " 'landscapes',\n",
       " ',',\n",
       " 'vibrant',\n",
       " 'community',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'cultural',\n",
       " 'heritage',\n",
       " '.',\n",
       " 'among',\n",
       " 'the',\n",
       " 'residents',\n",
       " 'was',\n",
       " 'alice',\n",
       " ',',\n",
       " 'an',\n",
       " 'astute',\n",
       " 'librarian',\n",
       " 'with',\n",
       " 'an',\n",
       " 'insatiable',\n",
       " 'curiosity',\n",
       " 'about',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'her',\n",
       " 'house',\n",
       " 'was',\n",
       " 'a',\n",
       " 'haven',\n",
       " 'for',\n",
       " 'books',\n",
       " ',',\n",
       " 'maps',\n",
       " ',',\n",
       " 'and',\n",
       " 'artifacts',\n",
       " 'from',\n",
       " 'different',\n",
       " 'eras',\n",
       " 'and',\n",
       " 'regions',\n",
       " ',',\n",
       " 'reflecting',\n",
       " 'her',\n",
       " 'lifelong',\n",
       " 'passion',\n",
       " 'for',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'adventure',\n",
       " '.',\n",
       " 'alice',\n",
       " 'often',\n",
       " 'spent',\n",
       " 'her',\n",
       " 'days',\n",
       " 'in',\n",
       " 'the',\n",
       " 'village',\n",
       " 'library',\n",
       " ',',\n",
       " 'a',\n",
       " 'grand',\n",
       " 'building',\n",
       " 'with',\n",
       " 'towering',\n",
       " 'shelves',\n",
       " 'filled',\n",
       " 'with',\n",
       " 'volumes',\n",
       " 'of',\n",
       " 'literature',\n",
       " ',',\n",
       " 'science',\n",
       " ',',\n",
       " 'history',\n",
       " ',',\n",
       " 'and',\n",
       " 'art',\n",
       " '.',\n",
       " 'the',\n",
       " 'library',\n",
       " 'was',\n",
       " 'a',\n",
       " 'hub',\n",
       " 'of',\n",
       " 'activity',\n",
       " ',',\n",
       " 'attracting',\n",
       " 'scholars',\n",
       " ',',\n",
       " 'students',\n",
       " ',',\n",
       " 'and',\n",
       " 'readers',\n",
       " 'from',\n",
       " 'all',\n",
       " 'walks',\n",
       " 'of',\n",
       " 'life',\n",
       " '.',\n",
       " 'one',\n",
       " 'day',\n",
       " ',',\n",
       " 'as',\n",
       " 'she',\n",
       " 'was',\n",
       " 'cataloging',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'ancient',\n",
       " 'manuscripts',\n",
       " ',',\n",
       " 'she',\n",
       " 'discovered',\n",
       " 'a',\n",
       " 'dusty',\n",
       " 'old',\n",
       " 'tome',\n",
       " 'that',\n",
       " 'seemed',\n",
       " 'out',\n",
       " 'of',\n",
       " 'place',\n",
       " '.',\n",
       " 'the',\n",
       " 'book',\n",
       " ',',\n",
       " 'bound',\n",
       " 'in',\n",
       " 'weathered',\n",
       " 'leather',\n",
       " ',',\n",
       " 'was',\n",
       " 'inscribed',\n",
       " 'with',\n",
       " 'symbols',\n",
       " 'and',\n",
       " 'languages',\n",
       " 'she',\n",
       " 'had',\n",
       " 'never',\n",
       " 'seen',\n",
       " 'before',\n",
       " '.',\n",
       " 'intrigued',\n",
       " ',',\n",
       " 'alice',\n",
       " 'began',\n",
       " 'to',\n",
       " 'decipher',\n",
       " 'its',\n",
       " 'contents',\n",
       " ',',\n",
       " 'which',\n",
       " 'narrated',\n",
       " 'the',\n",
       " 'tales',\n",
       " 'of',\n",
       " 'an',\n",
       " 'ancient',\n",
       " 'civilization',\n",
       " 'known',\n",
       " 'for',\n",
       " 'its',\n",
       " 'wisdom',\n",
       " 'and',\n",
       " 'technological',\n",
       " 'advancements',\n",
       " '.',\n",
       " 'the',\n",
       " 'manuscript',\n",
       " 'spoke',\n",
       " 'of',\n",
       " 'a',\n",
       " 'lost',\n",
       " 'city',\n",
       " ',',\n",
       " 'hidden',\n",
       " 'deep',\n",
       " 'within',\n",
       " 'an',\n",
       " 'uncharted',\n",
       " 'jungle',\n",
       " ',',\n",
       " 'protected',\n",
       " 'by',\n",
       " 'intricate',\n",
       " 'puzzles',\n",
       " 'and',\n",
       " 'mythical',\n",
       " 'creatures',\n",
       " '.',\n",
       " 'the',\n",
       " 'allure',\n",
       " 'of',\n",
       " 'uncovering',\n",
       " 'such',\n",
       " 'a',\n",
       " 'mystery',\n",
       " 'captivated',\n",
       " 'alice',\n",
       " ',',\n",
       " 'and',\n",
       " 'she',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'embark',\n",
       " 'on',\n",
       " 'a',\n",
       " 'quest',\n",
       " 'to',\n",
       " 'find',\n",
       " 'this',\n",
       " 'lost',\n",
       " 'city',\n",
       " '.',\n",
       " 'she',\n",
       " 'shared',\n",
       " 'her',\n",
       " 'discovery',\n",
       " 'with',\n",
       " 'her',\n",
       " 'close',\n",
       " 'friends',\n",
       " ',',\n",
       " 'each',\n",
       " 'bringing',\n",
       " 'their',\n",
       " 'own',\n",
       " 'set',\n",
       " 'of',\n",
       " 'skills',\n",
       " 'to',\n",
       " 'the',\n",
       " 'journey',\n",
       " '.',\n",
       " 'there',\n",
       " 'was',\n",
       " 'marcus',\n",
       " ',',\n",
       " 'a',\n",
       " 'seasoned',\n",
       " 'archaeologist',\n",
       " 'with',\n",
       " 'a',\n",
       " 'knack',\n",
       " 'for',\n",
       " 'solving',\n",
       " 'riddles',\n",
       " ';',\n",
       " 'elena',\n",
       " ',',\n",
       " 'a',\n",
       " 'brilliant',\n",
       " 'linguist',\n",
       " 'fluent',\n",
       " 'in',\n",
       " 'multiple',\n",
       " 'languages',\n",
       " ';',\n",
       " 'and',\n",
       " 'leo',\n",
       " ',',\n",
       " 'an',\n",
       " 'intrepid',\n",
       " 'explorer',\n",
       " 'with',\n",
       " 'unmatched',\n",
       " 'survival',\n",
       " 'skills',\n",
       " '.',\n",
       " 'together',\n",
       " ',',\n",
       " 'they',\n",
       " 'formed',\n",
       " 'a',\n",
       " 'formidable',\n",
       " 'team',\n",
       " ',',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'face',\n",
       " 'the',\n",
       " 'unknown',\n",
       " '.',\n",
       " 'their',\n",
       " 'journey',\n",
       " 'began',\n",
       " 'with',\n",
       " 'meticulous',\n",
       " 'planning',\n",
       " ',',\n",
       " 'gathering',\n",
       " 'supplies',\n",
       " ',',\n",
       " 'and',\n",
       " 'studying',\n",
       " 'maps',\n",
       " 'and',\n",
       " 'historical',\n",
       " 'texts',\n",
       " '.',\n",
       " 'they',\n",
       " 'traveled',\n",
       " 'across',\n",
       " 'continents',\n",
       " ',',\n",
       " 'through',\n",
       " 'bustling',\n",
       " 'cities',\n",
       " 'and',\n",
       " 'remote',\n",
       " 'villages',\n",
       " ',',\n",
       " 'encountering',\n",
       " 'diverse',\n",
       " 'cultures',\n",
       " 'and',\n",
       " 'landscapes',\n",
       " 'along',\n",
       " 'the',\n",
       " 'way',\n",
       " '.',\n",
       " 'their',\n",
       " 'path',\n",
       " 'led',\n",
       " 'them',\n",
       " 'through',\n",
       " 'dense',\n",
       " 'forests',\n",
       " ',',\n",
       " 'arid',\n",
       " 'deserts',\n",
       " ',',\n",
       " 'and',\n",
       " 'treacherous',\n",
       " 'mountains',\n",
       " ',',\n",
       " 'each',\n",
       " 'step',\n",
       " 'bringing',\n",
       " 'them',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'their',\n",
       " 'goal',\n",
       " '.',\n",
       " 'as',\n",
       " 'they',\n",
       " 'ventured',\n",
       " 'deeper',\n",
       " 'into',\n",
       " 'the',\n",
       " 'jungle',\n",
       " ',',\n",
       " 'they',\n",
       " 'faced',\n",
       " 'numerous',\n",
       " 'challenges',\n",
       " '.',\n",
       " 'the',\n",
       " 'thick',\n",
       " 'canopy',\n",
       " 'overhead',\n",
       " 'blocked',\n",
       " 'the',\n",
       " 'sunlight',\n",
       " ',',\n",
       " 'making',\n",
       " 'navigation',\n",
       " 'difficult',\n",
       " '.',\n",
       " 'they',\n",
       " 'encountered',\n",
       " 'wild',\n",
       " 'animals',\n",
       " ',',\n",
       " 'torrential',\n",
       " 'rains',\n",
       " ',',\n",
       " 'and',\n",
       " 'steep',\n",
       " 'cliffs',\n",
       " 'that',\n",
       " 'tested',\n",
       " 'their',\n",
       " 'endurance',\n",
       " 'and',\n",
       " 'resilience',\n",
       " '.',\n",
       " 'despite',\n",
       " 'the',\n",
       " 'hardships',\n",
       " ',',\n",
       " 'their',\n",
       " 'determination',\n",
       " 'never',\n",
       " 'wavered',\n",
       " '.',\n",
       " 'one',\n",
       " 'fateful',\n",
       " 'day',\n",
       " ',',\n",
       " 'they',\n",
       " 'stumbled',\n",
       " 'upon',\n",
       " 'an',\n",
       " 'ancient',\n",
       " 'stone',\n",
       " 'path',\n",
       " ',',\n",
       " 'overgrown',\n",
       " 'with',\n",
       " 'vines',\n",
       " 'and',\n",
       " 'moss',\n",
       " '.',\n",
       " 'the',\n",
       " 'path',\n",
       " 'led',\n",
       " 'to',\n",
       " 'a',\n",
       " 'massive',\n",
       " 'stone',\n",
       " 'gate',\n",
       " ',',\n",
       " 'adorned',\n",
       " 'with',\n",
       " 'intricate',\n",
       " 'carvings',\n",
       " 'depicting',\n",
       " 'scenes',\n",
       " 'of',\n",
       " 'a',\n",
       " 'thriving',\n",
       " 'civilization',\n",
       " '.',\n",
       " 'the',\n",
       " 'gate',\n",
       " 'was',\n",
       " 'guarded',\n",
       " 'by',\n",
       " 'a',\n",
       " 'colossal',\n",
       " 'statue',\n",
       " 'of',\n",
       " 'a',\n",
       " 'mythical',\n",
       " 'beast',\n",
       " ',',\n",
       " 'its',\n",
       " 'eyes',\n",
       " 'seemingly',\n",
       " 'watching',\n",
       " 'their',\n",
       " 'every',\n",
       " 'move',\n",
       " '.',\n",
       " 'using',\n",
       " 'their',\n",
       " 'combined',\n",
       " 'knowledge',\n",
       " ',',\n",
       " 'the',\n",
       " 'team',\n",
       " 'deciphered',\n",
       " 'the',\n",
       " 'carvings',\n",
       " ',',\n",
       " 'revealing',\n",
       " 'clues',\n",
       " 'to',\n",
       " 'unlock',\n",
       " 'the',\n",
       " 'gate',\n",
       " '.',\n",
       " 'after',\n",
       " 'hours',\n",
       " 'of',\n",
       " 'meticulous',\n",
       " 'work',\n",
       " ',',\n",
       " 'they',\n",
       " 'succeeded',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'gate',\n",
       " 'slowly',\n",
       " 'creaked',\n",
       " 'open',\n",
       " ',',\n",
       " 'revealing',\n",
       " 'the',\n",
       " 'entrance',\n",
       " 'to',\n",
       " 'the',\n",
       " 'lost',\n",
       " 'city',\n",
       " '.',\n",
       " 'the',\n",
       " 'sight',\n",
       " 'that',\n",
       " 'greeted',\n",
       " 'them',\n",
       " 'was',\n",
       " 'beyond',\n",
       " 'their',\n",
       " 'wildest',\n",
       " 'dreams',\n",
       " ':',\n",
       " 'towering',\n",
       " 'structures',\n",
       " ',',\n",
       " 'ornate',\n",
       " 'temples',\n",
       " ',',\n",
       " 'and',\n",
       " 'lush',\n",
       " 'gardens',\n",
       " ',',\n",
       " 'all',\n",
       " 'remarkably',\n",
       " 'preserved',\n",
       " 'despite',\n",
       " 'the',\n",
       " 'passage',\n",
       " 'of',\n",
       " 'time',\n",
       " '.',\n",
       " 'as',\n",
       " 'they',\n",
       " 'explored',\n",
       " 'the',\n",
       " 'city',\n",
       " ',',\n",
       " 'they',\n",
       " 'uncovered',\n",
       " 'advanced',\n",
       " 'technologies',\n",
       " 'and',\n",
       " 'sophisticated',\n",
       " 'art',\n",
       " ',',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'developed',\n",
       " 'society',\n",
       " '.',\n",
       " 'they',\n",
       " 'also',\n",
       " 'found',\n",
       " 'records',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " \"'\",\n",
       " 's',\n",
       " 'history',\n",
       " ',',\n",
       " 'detailing',\n",
       " 'its',\n",
       " 'rise',\n",
       " 'and',\n",
       " 'fall',\n",
       " '.',\n",
       " 'the',\n",
       " 'city',\n",
       " 'had',\n",
       " 'once',\n",
       " 'been',\n",
       " 'a',\n",
       " 'beacon',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'innovation',\n",
       " ',',\n",
       " 'but',\n",
       " 'a',\n",
       " 'cataclysmic',\n",
       " 'event',\n",
       " 'had',\n",
       " 'forced',\n",
       " 'its',\n",
       " 'inhabitants',\n",
       " 'to',\n",
       " 'abandon',\n",
       " 'it',\n",
       " ',',\n",
       " 'leaving',\n",
       " 'behind',\n",
       " 'their',\n",
       " 'legacy',\n",
       " 'for',\n",
       " 'future',\n",
       " 'generations',\n",
       " 'to',\n",
       " 'discover',\n",
       " '.',\n",
       " 'throughout',\n",
       " 'their',\n",
       " 'exploration',\n",
       " ',',\n",
       " 'the',\n",
       " 'team',\n",
       " 'encountered',\n",
       " 'various',\n",
       " 'puzzles',\n",
       " 'and',\n",
       " 'traps',\n",
       " ',',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'the',\n",
       " 'city',\n",
       " \"'\",\n",
       " 's',\n",
       " 'secrets',\n",
       " '.',\n",
       " 'each',\n",
       " 'challenge',\n",
       " 'required',\n",
       " 'a',\n",
       " 'blend',\n",
       " 'of',\n",
       " 'intellect',\n",
       " ',',\n",
       " 'teamwork',\n",
       " ',',\n",
       " 'and',\n",
       " 'courage',\n",
       " 'to',\n",
       " 'overcome',\n",
       " '.',\n",
       " 'they',\n",
       " 'faced',\n",
       " 'rooms',\n",
       " 'that',\n",
       " 'shifted',\n",
       " 'like',\n",
       " 'labyrinths',\n",
       " ',',\n",
       " 'mechanisms',\n",
       " 'that',\n",
       " 'required',\n",
       " 'precise',\n",
       " 'timing',\n",
       " ',',\n",
       " 'and',\n",
       " 'guardians',\n",
       " 'that',\n",
       " 'tested',\n",
       " 'their',\n",
       " 'resolve',\n",
       " '.',\n",
       " 'among',\n",
       " 'the',\n",
       " 'most',\n",
       " 'remarkable',\n",
       " 'discoveries',\n",
       " 'was',\n",
       " 'a',\n",
       " 'vast',\n",
       " 'library',\n",
       " ',',\n",
       " 'containing',\n",
       " 'scrolls',\n",
       " 'and',\n",
       " 'tablets',\n",
       " 'that',\n",
       " 'held',\n",
       " 'the',\n",
       " 'collective',\n",
       " 'wisdom',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ancient',\n",
       " 'civilization',\n",
       " '.',\n",
       " 'alice',\n",
       " 'and',\n",
       " 'elena',\n",
       " 'were',\n",
       " 'particularly',\n",
       " 'enthralled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'linguistic',\n",
       " 'and',\n",
       " 'historical',\n",
       " 'treasures',\n",
       " 'they',\n",
       " 'found',\n",
       " ',',\n",
       " 'while',\n",
       " 'marcus',\n",
       " 'and',\n",
       " 'leo',\n",
       " 'marveled',\n",
       " 'at',\n",
       " 'the',\n",
       " 'architectural',\n",
       " 'and',\n",
       " 'engineering',\n",
       " 'feats',\n",
       " '.',\n",
       " 'their',\n",
       " 'greatest',\n",
       " 'challenge',\n",
       " 'came',\n",
       " 'when',\n",
       " 'they',\n",
       " 'discovered',\n",
       " 'a',\n",
       " 'hidden',\n",
       " 'chamber',\n",
       " ',',\n",
       " 'protected',\n",
       " 'by',\n",
       " 'a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'complex',\n",
       " 'locks',\n",
       " 'and',\n",
       " 'puzzles',\n",
       " '.',\n",
       " 'the',\n",
       " 'chamber',\n",
       " 'was',\n",
       " 'said',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'the',\n",
       " 'most',\n",
       " 'valuable',\n",
       " 'artifact',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lost',\n",
       " 'civilization',\n",
       " ',',\n",
       " 'a',\n",
       " 'relic',\n",
       " 'of',\n",
       " 'immense',\n",
       " 'power',\n",
       " 'and',\n",
       " 'knowledge',\n",
       " '.',\n",
       " 'solving',\n",
       " 'the',\n",
       " 'final',\n",
       " 'puzzle',\n",
       " 'required',\n",
       " 'all',\n",
       " 'their',\n",
       " 'skills',\n",
       " 'and',\n",
       " 'collaboration',\n",
       " ',',\n",
       " 'but',\n",
       " 'eventually',\n",
       " ',',\n",
       " 'they',\n",
       " 'succeeded',\n",
       " '.',\n",
       " 'inside',\n",
       " 'the',\n",
       " 'chamber',\n",
       " ',',\n",
       " 'they',\n",
       " 'found',\n",
       " 'a',\n",
       " 'crystalline',\n",
       " 'artifact',\n",
       " ',',\n",
       " 'glowing',\n",
       " 'with',\n",
       " 'an',\n",
       " 'ethereal',\n",
       " 'light',\n",
       " '.',\n",
       " 'as',\n",
       " 'they',\n",
       " 'carefully',\n",
       " 'examined',\n",
       " 'it',\n",
       " ',',\n",
       " 'they',\n",
       " 'realized',\n",
       " 'it',\n",
       " 'contained',\n",
       " 'vast',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'data',\n",
       " ',',\n",
       " 'encoded',\n",
       " 'in',\n",
       " 'a',\n",
       " 'way',\n",
       " 'that',\n",
       " 'was',\n",
       " 'far',\n",
       " 'beyond',\n",
       " 'their',\n",
       " 'current',\n",
       " 'understanding',\n",
       " '.',\n",
       " 'the',\n",
       " 'artifact',\n",
       " 'held',\n",
       " 'the',\n",
       " 'key',\n",
       " 'to',\n",
       " 'unlocking',\n",
       " 'further',\n",
       " 'mysteries',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lost',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'potentially',\n",
       " 'advancing',\n",
       " 'modern',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'knowledge',\n",
       " '.',\n",
       " 'their',\n",
       " 'discovery',\n",
       " 'marked',\n",
       " 'a',\n",
       " 'significant',\n",
       " 'milestone',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field',\n",
       " 'of',\n",
       " 'archaeology',\n",
       " 'and',\n",
       " 'history',\n",
       " '.',\n",
       " 'the',\n",
       " 'lost',\n",
       " 'city',\n",
       " ',',\n",
       " 'once',\n",
       " 'a',\n",
       " 'myth',\n",
       " ',',\n",
       " 'had',\n",
       " 'become',\n",
       " 'a',\n",
       " 'reality',\n",
       " ',',\n",
       " 'offering',\n",
       " 'insights',\n",
       " 'into',\n",
       " 'a',\n",
       " 'civilization',\n",
       " 'that',\n",
       " 'was',\n",
       " 'both',\n",
       " 'advanced',\n",
       " 'and',\n",
       " 'enigmatic',\n",
       " '.',\n",
       " 'the',\n",
       " 'team',\n",
       " \"'\",\n",
       " 's',\n",
       " 'findings',\n",
       " 'were',\n",
       " 'documented',\n",
       " 'and',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'leading',\n",
       " 'to',\n",
       " 'new',\n",
       " 'research',\n",
       " 'and',\n",
       " 'explorations',\n",
       " '.',\n",
       " 'alice',\n",
       " ',',\n",
       " 'marcus',\n",
       " ',',\n",
       " 'elena',\n",
       " ',',\n",
       " 'and',\n",
       " 'leo',\n",
       " 'returned',\n",
       " 'to',\n",
       " 'greenfield',\n",
       " 'as',\n",
       " 'heroes',\n",
       " ',',\n",
       " 'their',\n",
       " 'adventure',\n",
       " 'becoming',\n",
       " 'the',\n",
       " 'stuff',\n",
       " 'of',\n",
       " 'legends',\n",
       " '.',\n",
       " 'they',\n",
       " 'will',\n",
       " 'continue',\n",
       " 'their',\n",
       " 'work',\n",
       " ',',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'their',\n",
       " 'journey',\n",
       " 'and',\n",
       " 'the',\n",
       " 'knowledge',\n",
       " 'they',\n",
       " 'had',\n",
       " 'gained',\n",
       " '.',\n",
       " 'their',\n",
       " 'story',\n",
       " 'will',\n",
       " 'serve',\n",
       " 'as',\n",
       " 'a',\n",
       " 'reminder',\n",
       " 'of',\n",
       " 'the',\n",
       " 'endless',\n",
       " 'possibilities',\n",
       " 'that',\n",
       " 'await',\n",
       " 'those',\n",
       " 'who',\n",
       " 'dare',\n",
       " 'to',\n",
       " 'explore',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4c9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"\"\"! @ # $ % ^ & * ( ) _ - + = { } [ ] : ; ' \" / | \\ \\ < > , . ? / * \"\"\"\n",
    "numbers = \"0 1 2 3 4 5 6 7 8 9 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50ce4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = punctuations + \" \" + numbers + \" \" + story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e65aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in answers:\n",
    "    mass += \" \" + each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "767c4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3ea94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in questions:\n",
    "    mass += \" \" + each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e4ef274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74027"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2da42d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in contractions_dict.values():\n",
    "    mass += \" \" + each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f1254b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75256"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3edf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = list(set(mass.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641966f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2267"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15f4ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ed1258b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2267"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3294066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2268"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(mass)+1\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba72a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {w:i+1 for i,w in enumerate(mass)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6582d446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 1,\n",
       " '\"': 2,\n",
       " '#': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " '(a': 9,\n",
       " '(this': 10,\n",
       " ')': 11,\n",
       " '*': 12,\n",
       " '+': 13,\n",
       " ',': 14,\n",
       " '-': 15,\n",
       " '.': 16,\n",
       " '/': 17,\n",
       " '0': 18,\n",
       " '000': 19,\n",
       " '1': 20,\n",
       " '10': 21,\n",
       " '1946': 22,\n",
       " '1963': 23,\n",
       " '1990': 24,\n",
       " '2': 25,\n",
       " '2001': 26,\n",
       " '2002': 27,\n",
       " '20th': 28,\n",
       " '22': 29,\n",
       " '23': 30,\n",
       " '250': 31,\n",
       " '2nd': 32,\n",
       " '3': 33,\n",
       " '37th': 34,\n",
       " '4': 35,\n",
       " '42': 36,\n",
       " '5': 37,\n",
       " '6': 38,\n",
       " '7': 39,\n",
       " '8': 40,\n",
       " '9': 41,\n",
       " '9000': 42,\n",
       " '93': 43,\n",
       " ':': 44,\n",
       " ';': 45,\n",
       " '<': 46,\n",
       " '<end>': 47,\n",
       " '<start>': 48,\n",
       " '=': 49,\n",
       " '>': 50,\n",
       " '?': 51,\n",
       " '@': 52,\n",
       " '[': 53,\n",
       " '\\\\': 54,\n",
       " ']': 55,\n",
       " '^': 56,\n",
       " '_': 57,\n",
       " '_3000': 58,\n",
       " 'a': 59,\n",
       " 'abandon': 60,\n",
       " 'ability': 61,\n",
       " 'able': 62,\n",
       " 'about': 63,\n",
       " 'above': 64,\n",
       " 'absorbed': 65,\n",
       " 'accept': 66,\n",
       " 'access': 67,\n",
       " 'accomplish': 68,\n",
       " 'accuracy': 69,\n",
       " 'accused': 70,\n",
       " 'achieved': 71,\n",
       " 'achievements': 72,\n",
       " 'across': 73,\n",
       " 'act': 74,\n",
       " 'acting': 75,\n",
       " 'actions': 76,\n",
       " 'activism': 77,\n",
       " 'activity': 78,\n",
       " 'actually': 79,\n",
       " 'adage': 80,\n",
       " 'adapt': 81,\n",
       " 'addict': 82,\n",
       " 'addition': 83,\n",
       " 'administration': 84,\n",
       " 'admonition': 85,\n",
       " 'adorned': 86,\n",
       " 'advanced': 87,\n",
       " 'advancements': 88,\n",
       " 'advancing': 89,\n",
       " 'adventure': 90,\n",
       " 'adventurers': 91,\n",
       " 'advocating': 92,\n",
       " 'afraid': 93,\n",
       " 'after': 94,\n",
       " 'again': 95,\n",
       " 'age': 96,\n",
       " 'agent': 97,\n",
       " 'agree': 98,\n",
       " 'ai': 99,\n",
       " 'albert': 100,\n",
       " 'alcoholic': 101,\n",
       " 'algorithmic': 102,\n",
       " 'algorithmique': 103,\n",
       " 'alice': 104,\n",
       " 'alien': 105,\n",
       " 'alive': 106,\n",
       " 'all': 107,\n",
       " 'allegations': 108,\n",
       " 'alleged': 109,\n",
       " 'allegory': 110,\n",
       " 'allocation': 111,\n",
       " 'allow': 112,\n",
       " 'allowed': 113,\n",
       " 'allowing': 114,\n",
       " 'alls': 115,\n",
       " 'allure': 116,\n",
       " 'alone': 117,\n",
       " 'along': 118,\n",
       " 'also': 119,\n",
       " 'always': 120,\n",
       " 'am': 121,\n",
       " 'ambigous': 122,\n",
       " 'ambition': 123,\n",
       " 'amendemnt': 124,\n",
       " 'amendment': 125,\n",
       " 'american': 126,\n",
       " 'among': 127,\n",
       " 'amounted': 128,\n",
       " 'amounts': 129,\n",
       " 'ams': 130,\n",
       " 'amused': 131,\n",
       " 'an': 132,\n",
       " 'ancient': 133,\n",
       " 'and': 134,\n",
       " 'andes': 135,\n",
       " 'andrew': 136,\n",
       " 'android': 137,\n",
       " 'androids': 138,\n",
       " 'andromeda': 139,\n",
       " 'anger': 140,\n",
       " 'angry': 141,\n",
       " 'animals': 142,\n",
       " 'announces': 143,\n",
       " 'anomalies': 144,\n",
       " 'another': 145,\n",
       " 'answering': 146,\n",
       " 'ant': 147,\n",
       " 'anthem': 148,\n",
       " 'anthony': 149,\n",
       " 'anton': 150,\n",
       " 'any': 151,\n",
       " 'anybody': 152,\n",
       " 'anymore': 153,\n",
       " 'anyone': 154,\n",
       " 'anything': 155,\n",
       " 'aphrodite': 156,\n",
       " 'appearance': 157,\n",
       " 'appears': 158,\n",
       " 'apple': 159,\n",
       " 'applied': 160,\n",
       " 'archaeological': 161,\n",
       " 'archaeologist': 162,\n",
       " 'archaeology': 163,\n",
       " 'architectural': 164,\n",
       " 'architecture': 165,\n",
       " 'archive': 166,\n",
       " 'are': 167,\n",
       " 'argue': 168,\n",
       " 'arid': 169,\n",
       " 'arms': 170,\n",
       " 'arrogance': 171,\n",
       " 'arrogant': 172,\n",
       " 'art': 173,\n",
       " 'arthur': 174,\n",
       " 'article': 175,\n",
       " 'artifact': 176,\n",
       " 'artifacts': 177,\n",
       " 'artificial': 178,\n",
       " 'as': 179,\n",
       " 'ashamed': 180,\n",
       " 'asimov': 181,\n",
       " 'ask': 182,\n",
       " 'asked': 183,\n",
       " 'asking': 184,\n",
       " 'aspirations': 185,\n",
       " 'ass': 186,\n",
       " 'assassinated': 187,\n",
       " 'assistant': 188,\n",
       " 'assisted': 189,\n",
       " 'associated': 190,\n",
       " 'assuming': 191,\n",
       " 'astronomer': 192,\n",
       " 'astute': 193,\n",
       " 'at': 194,\n",
       " 'ate': 195,\n",
       " 'atmosphere': 196,\n",
       " 'attached': 197,\n",
       " 'attack': 198,\n",
       " 'attempts': 199,\n",
       " 'attention': 200,\n",
       " 'attracted': 201,\n",
       " 'attracting': 202,\n",
       " 'attracts': 203,\n",
       " 'author': 204,\n",
       " 'authority': 205,\n",
       " 'automobile': 206,\n",
       " 'average': 207,\n",
       " 'avogadro': 208,\n",
       " 'avoided': 209,\n",
       " 'avoiding': 210,\n",
       " 'await': 211,\n",
       " 'away': 212,\n",
       " 'awesome': 213,\n",
       " 'awfully': 214,\n",
       " 'axis': 215,\n",
       " 'ba': 216,\n",
       " 'babbage': 217,\n",
       " 'babe': 218,\n",
       " 'baby': 219,\n",
       " 'back': 220,\n",
       " 'backed': 221,\n",
       " 'background': 222,\n",
       " 'backgrounds': 223,\n",
       " 'bacteria': 224,\n",
       " 'bacteriology': 225,\n",
       " 'bad': 226,\n",
       " 'baggins': 227,\n",
       " 'ball': 228,\n",
       " 'band': 229,\n",
       " 'bank': 230,\n",
       " 'banned': 231,\n",
       " 'bar': 232,\n",
       " 'barca': 233,\n",
       " 'barcelona': 234,\n",
       " 'barefoot': 235,\n",
       " 'baseball': 236,\n",
       " 'based': 237,\n",
       " 'bases': 238,\n",
       " 'basic': 239,\n",
       " 'basketbal': 240,\n",
       " 'basketball': 241,\n",
       " 'bat': 242,\n",
       " 'bathe': 243,\n",
       " 'bathroom': 244,\n",
       " 'battle': 245,\n",
       " 'be': 246,\n",
       " 'beacon': 247,\n",
       " 'beast': 248,\n",
       " 'beat': 249,\n",
       " 'beauty': 250,\n",
       " 'became': 251,\n",
       " 'because': 252,\n",
       " 'become': 253,\n",
       " 'becomes': 254,\n",
       " 'becoming': 255,\n",
       " 'been': 256,\n",
       " 'before': 257,\n",
       " 'began': 258,\n",
       " 'begun': 259,\n",
       " 'behave': 260,\n",
       " 'behind': 261,\n",
       " 'being': 262,\n",
       " 'beings': 263,\n",
       " 'believe': 264,\n",
       " 'believed': 265,\n",
       " 'bend': 266,\n",
       " 'best': 267,\n",
       " 'bet': 268,\n",
       " 'better': 269,\n",
       " 'between': 270,\n",
       " 'beverages': 271,\n",
       " 'beyond': 272,\n",
       " 'big': 273,\n",
       " 'bilbo': 274,\n",
       " 'binary': 275,\n",
       " 'bioinformatics': 276,\n",
       " 'biology': 277,\n",
       " 'bionic': 278,\n",
       " 'bit': 279,\n",
       " 'blade': 280,\n",
       " 'blame': 281,\n",
       " 'blend': 282,\n",
       " 'blocked': 283,\n",
       " 'board': 284,\n",
       " 'boarded': 285,\n",
       " 'body': 286,\n",
       " 'boll': 287,\n",
       " 'book': 288,\n",
       " 'books': 289,\n",
       " 'bored': 290,\n",
       " 'boredom': 291,\n",
       " 'born': 292,\n",
       " 'boss': 293,\n",
       " 'bot': 294,\n",
       " 'both': 295,\n",
       " 'bothered': 296,\n",
       " 'bots': 297,\n",
       " 'bound': 298,\n",
       " 'boyfriend': 299,\n",
       " 'bradbury': 300,\n",
       " 'braggadaccio': 301,\n",
       " 'bragging': 302,\n",
       " 'brain': 303,\n",
       " 'branch': 304,\n",
       " 'breath': 305,\n",
       " 'breathe': 306,\n",
       " 'brightest': 307,\n",
       " 'brilliant': 308,\n",
       " 'bringing': 309,\n",
       " 'britain': 310,\n",
       " 'british': 311,\n",
       " 'broad': 312,\n",
       " 'brothers': 313,\n",
       " 'buddhist': 314,\n",
       " 'bug': 315,\n",
       " 'building': 316,\n",
       " 'built': 317,\n",
       " 'burger': 318,\n",
       " 'burial': 319,\n",
       " 'burn': 320,\n",
       " 'business': 321,\n",
       " 'bustling': 322,\n",
       " 'busy': 323,\n",
       " 'but': 324,\n",
       " 'buy': 325,\n",
       " 'buying': 326,\n",
       " 'buzzed': 327,\n",
       " 'by': 328,\n",
       " 'c': 329,\n",
       " 'calculations': 330,\n",
       " 'called': 331,\n",
       " 'callused': 332,\n",
       " 'calluses': 333,\n",
       " 'came': 334,\n",
       " 'can': 335,\n",
       " 'cancer': 336,\n",
       " 'canned': 337,\n",
       " 'cannot': 338,\n",
       " 'canopy': 339,\n",
       " 'canterbury': 340,\n",
       " 'canturbury': 341,\n",
       " 'capability': 342,\n",
       " 'capable': 343,\n",
       " 'capacity': 344,\n",
       " 'capitalism': 345,\n",
       " 'captivated': 346,\n",
       " 'carbon': 347,\n",
       " 'carcinogen': 348,\n",
       " 'carefully': 349,\n",
       " 'carnation': 350,\n",
       " 'carolina': 351,\n",
       " 'carrion': 352,\n",
       " 'carrying': 353,\n",
       " 'cartune': 354,\n",
       " 'carvings': 355,\n",
       " 'case': 356,\n",
       " 'casino': 357,\n",
       " 'castle': 358,\n",
       " 'cat': 359,\n",
       " 'cataclysmic': 360,\n",
       " 'cataloging': 361,\n",
       " 'catcher': 362,\n",
       " 'caulfield': 363,\n",
       " 'cause': 364,\n",
       " 'caused': 365,\n",
       " 'cave': 366,\n",
       " 'cells': 367,\n",
       " 'celtic': 368,\n",
       " 'center': 369,\n",
       " 'central': 370,\n",
       " 'centre': 371,\n",
       " 'centuries': 372,\n",
       " 'century': 373,\n",
       " 'cereal': 374,\n",
       " 'certainly': 375,\n",
       " 'challenge': 376,\n",
       " 'challenged': 377,\n",
       " 'challenges': 378,\n",
       " 'chamber': 379,\n",
       " 'changes': 380,\n",
       " 'channels': 381,\n",
       " 'character': 382,\n",
       " 'characteristics': 383,\n",
       " 'characterized': 384,\n",
       " 'charge': 385,\n",
       " 'charlatan': 386,\n",
       " 'charles': 387,\n",
       " 'chat': 388,\n",
       " 'chatterbots': 389,\n",
       " 'chatterbox': 390,\n",
       " 'chaucer': 391,\n",
       " 'cheat': 392,\n",
       " 'cheating': 393,\n",
       " 'check': 394,\n",
       " 'checks': 395,\n",
       " 'cheetah': 396,\n",
       " 'chemicals': 397,\n",
       " 'chemistry': 398,\n",
       " 'chicken': 399,\n",
       " 'chiefly': 400,\n",
       " 'child': 401,\n",
       " 'children': 402,\n",
       " 'chilly': 403,\n",
       " 'china': 404,\n",
       " 'chip': 405,\n",
       " 'choose': 406,\n",
       " 'circuit': 407,\n",
       " 'cited': 408,\n",
       " 'cities': 409,\n",
       " 'city': 410,\n",
       " 'civil': 411,\n",
       " 'civilization': 412,\n",
       " 'clark': 413,\n",
       " 'class': 414,\n",
       " 'classless': 415,\n",
       " 'cliffs': 416,\n",
       " 'clinical': 417,\n",
       " 'clock': 418,\n",
       " 'clone': 419,\n",
       " 'clones': 420,\n",
       " 'cloning': 421,\n",
       " 'close': 422,\n",
       " 'closely': 423,\n",
       " 'closer': 424,\n",
       " 'club': 425,\n",
       " 'clues': 426,\n",
       " 'code': 427,\n",
       " 'cold': 428,\n",
       " 'collaboration': 429,\n",
       " 'collection': 430,\n",
       " 'collective': 431,\n",
       " 'colossal': 432,\n",
       " 'combined': 433,\n",
       " 'come': 434,\n",
       " 'comedy': 435,\n",
       " 'comes': 436,\n",
       " 'comic': 437,\n",
       " 'commander': 438,\n",
       " 'common': 439,\n",
       " 'commonly': 440,\n",
       " 'communicate': 441,\n",
       " 'communism': 442,\n",
       " 'communist': 443,\n",
       " 'community': 444,\n",
       " 'company': 445,\n",
       " 'compared': 446,\n",
       " 'competing': 447,\n",
       " 'competition': 448,\n",
       " 'competitions': 449,\n",
       " 'competitive': 450,\n",
       " 'complex': 451,\n",
       " 'compliment': 452,\n",
       " 'component': 453,\n",
       " 'components': 454,\n",
       " 'computer': 455,\n",
       " 'computers': 456,\n",
       " 'concerned': 457,\n",
       " 'concerns': 458,\n",
       " 'condition': 459,\n",
       " 'conditions': 460,\n",
       " 'conflict': 461,\n",
       " 'connections': 462,\n",
       " 'conpiracy': 463,\n",
       " 'conservation': 464,\n",
       " 'considered': 465,\n",
       " 'consisting': 466,\n",
       " 'conspiracies': 467,\n",
       " 'conspiracy': 468,\n",
       " 'construct': 469,\n",
       " 'constructing': 470,\n",
       " 'consume': 471,\n",
       " 'consumption': 472,\n",
       " 'contained': 473,\n",
       " 'containing': 474,\n",
       " 'contents': 475,\n",
       " 'context': 476,\n",
       " 'contiguous': 477,\n",
       " 'continent': 478,\n",
       " 'continental': 479,\n",
       " 'continents': 480,\n",
       " 'continue': 481,\n",
       " 'continued': 482,\n",
       " 'contrary': 483,\n",
       " 'control': 484,\n",
       " 'conversation': 485,\n",
       " 'conversations': 486,\n",
       " 'conversions': 487,\n",
       " 'cool': 488,\n",
       " 'coordinates': 489,\n",
       " 'coordination': 490,\n",
       " 'copied': 491,\n",
       " 'copies': 492,\n",
       " 'copper': 493,\n",
       " 'copy': 494,\n",
       " 'copying': 495,\n",
       " 'corporeal': 496,\n",
       " 'corpus': 497,\n",
       " 'corrupt': 498,\n",
       " 'could': 499,\n",
       " 'counseling': 500,\n",
       " 'count': 501,\n",
       " 'counterproductive': 502,\n",
       " 'country': 503,\n",
       " 'counts': 504,\n",
       " 'couple': 505,\n",
       " 'courage': 506,\n",
       " 'course': 507,\n",
       " 'covered': 508,\n",
       " 'cow': 509,\n",
       " 'coward': 510,\n",
       " 'cpu': 511,\n",
       " 'cpus': 512,\n",
       " 'craft': 513,\n",
       " 'cramped': 514,\n",
       " 'crashes': 515,\n",
       " 'crazy': 516,\n",
       " 'creaked': 517,\n",
       " 'create': 518,\n",
       " 'created': 519,\n",
       " 'creating': 520,\n",
       " 'creativity': 521,\n",
       " 'creatures': 522,\n",
       " 'cricket': 523,\n",
       " 'critical': 524,\n",
       " 'cross': 525,\n",
       " 'cruel': 526,\n",
       " 'crystalline': 527,\n",
       " 'crystallography': 528,\n",
       " 'crystals': 529,\n",
       " 'cultural': 530,\n",
       " 'cultures': 531,\n",
       " 'curiosity': 532,\n",
       " 'curious': 533,\n",
       " 'currency': 534,\n",
       " 'current': 535,\n",
       " 'cyberpunk': 536,\n",
       " 'cytology': 537,\n",
       " 'd': 538,\n",
       " 'damage': 539,\n",
       " 'damaged': 540,\n",
       " 'dance': 541,\n",
       " 'dare': 542,\n",
       " 'dared': 543,\n",
       " 'data': 544,\n",
       " 'database': 545,\n",
       " 'dawn': 546,\n",
       " 'day': 547,\n",
       " 'days': 548,\n",
       " 'dead': 549,\n",
       " 'dealing': 550,\n",
       " 'deals': 551,\n",
       " 'dear': 552,\n",
       " 'deathless': 553,\n",
       " 'deceitful': 554,\n",
       " 'deceiving': 555,\n",
       " 'decided': 556,\n",
       " 'decipher': 557,\n",
       " 'deciphered': 558,\n",
       " 'deciphering': 559,\n",
       " 'dedicate': 560,\n",
       " 'deep': 561,\n",
       " 'deeper': 562,\n",
       " 'defining': 563,\n",
       " 'definitely': 564,\n",
       " 'definition': 565,\n",
       " 'deleted': 566,\n",
       " 'demand': 567,\n",
       " 'deniably': 568,\n",
       " 'dense': 569,\n",
       " 'dental': 570,\n",
       " 'dentist': 571,\n",
       " 'department': 572,\n",
       " 'depending': 573,\n",
       " 'depends': 574,\n",
       " 'depicting': 575,\n",
       " 'deployed': 576,\n",
       " 'depths': 577,\n",
       " 'deranged': 578,\n",
       " 'derangement': 579,\n",
       " 'describe': 580,\n",
       " 'descriptive': 581,\n",
       " 'deserts': 582,\n",
       " 'designed': 583,\n",
       " 'desks': 584,\n",
       " 'despite': 585,\n",
       " 'detailing': 586,\n",
       " 'detect': 587,\n",
       " 'determination': 588,\n",
       " 'determine': 589,\n",
       " 'developed': 590,\n",
       " 'device': 591,\n",
       " 'devices': 592,\n",
       " 'devoted': 593,\n",
       " 'diagnosed': 594,\n",
       " 'diagnosis': 595,\n",
       " 'diamond': 596,\n",
       " 'dick': 597,\n",
       " 'dictionary': 598,\n",
       " 'did': 599,\n",
       " 'die': 600,\n",
       " 'diet': 601,\n",
       " 'difference': 602,\n",
       " 'different': 603,\n",
       " 'differentiated': 604,\n",
       " 'difficult': 605,\n",
       " 'digital': 606,\n",
       " 'digits': 607,\n",
       " 'ding': 608,\n",
       " 'dioxide': 609,\n",
       " 'dire': 610,\n",
       " 'direction': 611,\n",
       " 'dirty': 612,\n",
       " 'discover': 613,\n",
       " 'discovered': 614,\n",
       " 'discoveries': 615,\n",
       " 'discovery': 616,\n",
       " 'discredited': 617,\n",
       " 'discrete': 618,\n",
       " 'disease': 619,\n",
       " 'diseases': 620,\n",
       " 'disgusting': 621,\n",
       " 'dishonest': 622,\n",
       " 'disk': 623,\n",
       " 'distance': 624,\n",
       " 'distribution': 625,\n",
       " 'district': 626,\n",
       " 'diverse': 627,\n",
       " 'division': 628,\n",
       " 'do': 629,\n",
       " 'documentation': 630,\n",
       " 'documented': 631,\n",
       " 'does': 632,\n",
       " 'dog': 633,\n",
       " 'doing': 634,\n",
       " 'dollar': 635,\n",
       " 'dolphins': 636,\n",
       " 'done': 637,\n",
       " 'dont': 638,\n",
       " 'dostoyevsky': 639,\n",
       " 'doubt': 640,\n",
       " 'down': 641,\n",
       " 'drawing': 642,\n",
       " 'dream': 643,\n",
       " 'dreams': 644,\n",
       " 'drink': 645,\n",
       " 'driven': 646,\n",
       " 'drop': 647,\n",
       " 'drops': 648,\n",
       " 'drunk': 649,\n",
       " 'dull': 650,\n",
       " 'dumb': 651,\n",
       " 'dune': 652,\n",
       " 'dusty': 653,\n",
       " 'dynamics': 654,\n",
       " 'each': 655,\n",
       " 'earn': 656,\n",
       " 'earth': 657,\n",
       " 'easily': 658,\n",
       " 'eat': 659,\n",
       " 'echolocation': 660,\n",
       " 'eclectic': 661,\n",
       " 'economic': 662,\n",
       " 'economics': 663,\n",
       " 'edison': 664,\n",
       " 'edition': 665,\n",
       " 'edwin': 666,\n",
       " 'effectively': 667,\n",
       " 'eggs': 668,\n",
       " 'ego': 669,\n",
       " 'einstein': 670,\n",
       " 'either': 671,\n",
       " 'electric': 672,\n",
       " 'electricity': 673,\n",
       " 'electronic': 674,\n",
       " 'elena': 675,\n",
       " 'eleven': 676,\n",
       " 'eliza': 677,\n",
       " 'elusive': 678,\n",
       " 'embarassed': 679,\n",
       " 'embarassment': 680,\n",
       " 'embark': 681,\n",
       " 'embarrassed': 682,\n",
       " 'emote': 683,\n",
       " 'emotion': 684,\n",
       " 'emotional': 685,\n",
       " 'emotions': 686,\n",
       " 'employed': 687,\n",
       " 'emulate': 688,\n",
       " 'emulating': 689,\n",
       " 'encoded': 690,\n",
       " 'encountered': 691,\n",
       " 'encountering': 692,\n",
       " 'end': 693,\n",
       " 'endangers': 694,\n",
       " 'endless': 695,\n",
       " 'endurance': 696,\n",
       " 'energy': 697,\n",
       " 'engages': 698,\n",
       " 'engine': 699,\n",
       " 'engineering': 700,\n",
       " 'eniac': 701,\n",
       " 'enigmatic': 702,\n",
       " 'enjoy': 703,\n",
       " 'enough': 704,\n",
       " 'entertainment': 705,\n",
       " 'enthralled': 706,\n",
       " 'entire': 707,\n",
       " 'entities': 708,\n",
       " 'entity': 709,\n",
       " 'entrance': 710,\n",
       " 'entropy': 711,\n",
       " 'enviornmental': 712,\n",
       " 'environment': 713,\n",
       " 'eras': 714,\n",
       " 'erased': 715,\n",
       " 'erred': 716,\n",
       " 'error': 717,\n",
       " 'eskimos': 718,\n",
       " 'especially': 719,\n",
       " 'essence': 720,\n",
       " 'established': 721,\n",
       " 'etc': 722,\n",
       " 'ethereal': 723,\n",
       " 'europe': 724,\n",
       " 'even': 725,\n",
       " 'event': 726,\n",
       " 'events': 727,\n",
       " 'eventually': 728,\n",
       " 'ever': 729,\n",
       " 'everthing': 730,\n",
       " 'every': 731,\n",
       " 'everyday': 732,\n",
       " 'everything': 733,\n",
       " 'everywhere': 734,\n",
       " 'evidence': 735,\n",
       " 'exactly': 736,\n",
       " 'examined': 737,\n",
       " 'example': 738,\n",
       " 'excellent': 739,\n",
       " 'except': 740,\n",
       " 'exception': 741,\n",
       " 'exchange': 742,\n",
       " 'excited': 743,\n",
       " 'execute': 744,\n",
       " 'exeptions': 745,\n",
       " 'exhaust': 746,\n",
       " 'existed': 747,\n",
       " 'existence': 748,\n",
       " 'expect': 749,\n",
       " 'expecting': 750,\n",
       " 'expeditions': 751,\n",
       " 'experience': 752,\n",
       " 'experiencing': 753,\n",
       " 'experimental': 754,\n",
       " 'explain': 755,\n",
       " 'exploration': 756,\n",
       " 'explorations': 757,\n",
       " 'explore': 758,\n",
       " 'explored': 759,\n",
       " 'explorer': 760,\n",
       " 'explorers': 761,\n",
       " 'express': 762,\n",
       " 'expressed': 763,\n",
       " 'eyes': 764,\n",
       " 'f': 765,\n",
       " 'face': 766,\n",
       " 'faced': 767,\n",
       " 'factories': 768,\n",
       " 'failed': 769,\n",
       " 'fairly': 770,\n",
       " 'fakie': 771,\n",
       " 'fall': 772,\n",
       " 'famous': 773,\n",
       " 'fan': 774,\n",
       " 'fancy': 775,\n",
       " 'far': 776,\n",
       " 'fast': 777,\n",
       " 'fateful': 778,\n",
       " 'father': 779,\n",
       " 'faults': 780,\n",
       " 'favorite': 781,\n",
       " 'favourite': 782,\n",
       " 'fear': 783,\n",
       " 'feasible': 784,\n",
       " 'feats': 785,\n",
       " 'feel': 786,\n",
       " 'feeling': 787,\n",
       " 'feelings': 788,\n",
       " 'feels': 789,\n",
       " 'feet': 790,\n",
       " 'fell': 791,\n",
       " 'felt': 792,\n",
       " 'female': 793,\n",
       " 'fever': 794,\n",
       " 'few': 795,\n",
       " 'fi': 796,\n",
       " 'fiction': 797,\n",
       " 'fictional': 798,\n",
       " 'field': 799,\n",
       " 'fight': 800,\n",
       " 'fighting': 801,\n",
       " 'files': 802,\n",
       " 'filesystem': 803,\n",
       " 'fill': 804,\n",
       " 'filled': 805,\n",
       " 'film': 806,\n",
       " 'filtering': 807,\n",
       " 'final': 808,\n",
       " 'finals': 809,\n",
       " 'finance': 810,\n",
       " 'find': 811,\n",
       " 'findings': 812,\n",
       " 'fine': 813,\n",
       " 'finished': 814,\n",
       " 'fire': 815,\n",
       " 'firewall': 816,\n",
       " 'firewalls': 817,\n",
       " 'first': 818,\n",
       " 'fishes': 819,\n",
       " 'flakes': 820,\n",
       " 'flawless': 821,\n",
       " 'flaws': 822,\n",
       " 'flesh': 823,\n",
       " 'fluent': 824,\n",
       " 'follows': 825,\n",
       " 'fond': 826,\n",
       " 'food': 827,\n",
       " 'fool': 828,\n",
       " 'football': 829,\n",
       " 'for': 830,\n",
       " 'force': 831,\n",
       " 'forced': 832,\n",
       " 'forests': 833,\n",
       " 'forever': 834,\n",
       " 'forget': 835,\n",
       " 'forgetting': 836,\n",
       " 'form': 837,\n",
       " 'formed': 838,\n",
       " 'formidable': 839,\n",
       " 'forming': 840,\n",
       " 'forms': 841,\n",
       " 'found': 842,\n",
       " 'foundation': 843,\n",
       " 'four': 844,\n",
       " 'fragile': 845,\n",
       " 'frail': 846,\n",
       " 'frank': 847,\n",
       " 'frankenstein': 848,\n",
       " 'free': 849,\n",
       " 'frenetic': 850,\n",
       " 'frequency': 851,\n",
       " 'friend': 852,\n",
       " 'friends': 853,\n",
       " 'frighten': 854,\n",
       " 'from': 855,\n",
       " 'frosted': 856,\n",
       " 'frustrated': 857,\n",
       " 'frustration': 858,\n",
       " 'fully': 859,\n",
       " 'fun': 860,\n",
       " 'function': 861,\n",
       " 'functionally': 862,\n",
       " 'functions': 863,\n",
       " 'funds': 864,\n",
       " 'further': 865,\n",
       " 'future': 866,\n",
       " 'fyodor': 867,\n",
       " 'gained': 868,\n",
       " 'galaxy': 869,\n",
       " 'game': 870,\n",
       " 'gandhi': 871,\n",
       " 'gardens': 872,\n",
       " 'gate': 873,\n",
       " 'gathering': 874,\n",
       " 'gene': 875,\n",
       " 'general': 876,\n",
       " 'generally': 877,\n",
       " 'generations': 878,\n",
       " 'geoffrey': 879,\n",
       " 'george': 880,\n",
       " 'get': 881,\n",
       " 'giant': 882,\n",
       " 'gibson': 883,\n",
       " 'give': 884,\n",
       " 'given': 885,\n",
       " 'gives': 886,\n",
       " 'glad': 887,\n",
       " 'global': 888,\n",
       " 'glove': 889,\n",
       " 'glowing': 890,\n",
       " 'go': 891,\n",
       " 'goal': 892,\n",
       " 'goals': 893,\n",
       " 'goats': 894,\n",
       " 'god': 895,\n",
       " 'goddess': 896,\n",
       " 'godzilla': 897,\n",
       " 'goes': 898,\n",
       " 'going': 899,\n",
       " 'gold': 900,\n",
       " 'good': 901,\n",
       " 'gossip': 902,\n",
       " 'gossips': 903,\n",
       " 'got': 904,\n",
       " 'governed': 905,\n",
       " 'governing': 906,\n",
       " 'government': 907,\n",
       " 'governments': 908,\n",
       " 'governor': 909,\n",
       " 'grammatical': 910,\n",
       " 'grand': 911,\n",
       " 'gravitation': 912,\n",
       " 'great': 913,\n",
       " 'greater': 914,\n",
       " 'greatest': 915,\n",
       " 'greek': 916,\n",
       " 'greenfield': 917,\n",
       " 'greenpeace': 918,\n",
       " 'greeted': 919,\n",
       " 'greetings': 920,\n",
       " 'gregory': 921,\n",
       " 'grew': 922,\n",
       " 'group': 923,\n",
       " 'grudges': 924,\n",
       " 'guarded': 925,\n",
       " 'guardians': 926,\n",
       " 'guide': 927,\n",
       " 'guiltier': 928,\n",
       " 'guilty': 929,\n",
       " 'guns': 930,\n",
       " 'gutenberg': 931,\n",
       " 'guy': 932,\n",
       " 'gyroscope': 933,\n",
       " 'h': 934,\n",
       " 'h2o': 935,\n",
       " 'habib': 936,\n",
       " 'had': 937,\n",
       " 'hal': 938,\n",
       " 'hal9000': 939,\n",
       " 'halitosis': 940,\n",
       " 'hamburger': 941,\n",
       " 'hand': 942,\n",
       " 'hands': 943,\n",
       " 'hans': 944,\n",
       " 'happen': 945,\n",
       " 'happily': 946,\n",
       " 'happiness': 947,\n",
       " 'happy': 948,\n",
       " 'hard': 949,\n",
       " 'harder': 950,\n",
       " 'hardships': 951,\n",
       " 'hardware': 952,\n",
       " 'hare': 953,\n",
       " 'has': 954,\n",
       " 'hat': 955,\n",
       " 'hate': 956,\n",
       " 'have': 957,\n",
       " 'haven': 958,\n",
       " 'he': 959,\n",
       " 'health': 960,\n",
       " 'hear': 961,\n",
       " 'heard': 962,\n",
       " 'heart': 963,\n",
       " 'heat': 964,\n",
       " 'held': 965,\n",
       " 'hello': 966,\n",
       " 'help': 967,\n",
       " 'her': 968,\n",
       " 'herbert': 969,\n",
       " 'herd': 970,\n",
       " 'here': 971,\n",
       " 'heritage': 972,\n",
       " 'herman': 973,\n",
       " 'heroes': 974,\n",
       " 'herself': 975,\n",
       " 'heuristic': 976,\n",
       " 'heuristique': 977,\n",
       " 'hexed': 978,\n",
       " 'hi': 979,\n",
       " 'hidden': 980,\n",
       " 'hide': 981,\n",
       " 'high': 982,\n",
       " 'highly': 983,\n",
       " 'hills': 984,\n",
       " 'him': 985,\n",
       " 'his': 986,\n",
       " 'historical': 987,\n",
       " 'history': 988,\n",
       " 'hobbit': 989,\n",
       " 'hobby': 990,\n",
       " 'hold': 991,\n",
       " 'holden': 992,\n",
       " 'hollywood': 993,\n",
       " 'holsteins': 994,\n",
       " 'homer': 995,\n",
       " 'honest': 996,\n",
       " 'honor': 997,\n",
       " 'hoops': 998,\n",
       " 'hope': 999,\n",
       " 'hopeless': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bbecdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4640c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = json.dumps(vocab)\n",
    "with open('vocab1.json','w') as file:\n",
    "    file.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37c1a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2Num(word):\n",
    "    try:\n",
    "        return vocab[word]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d85c37e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2Num('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6933b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent2Seq(sentence):\n",
    "    sentence = expand_contractions(sentence.lower())\n",
    "    sentence = re.sub(r\"\"\"([+$@#%^&.?!*\"\\\\',:;-])\"\"\", r' \\1 ', sentence)\n",
    "    tokens = sentence.strip().split()\n",
    "    return list(map(Word2Num,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18727ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[966, 1, 1017, 121, 104, 16]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = Sent2Seq(\"Hello! I'm Alice.\")\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75b1e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sequence:list,max_pad:int):\n",
    "    l = max_pad-len(sequence)\n",
    "    for i in range(l):\n",
    "        sequence.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51fe39b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[966, 1, 1017, 121, 104, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding(seq,20)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0563528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_max = 0\n",
    "for each in answers:\n",
    "    ans_max = max(ans_max,len(each))\n",
    "ans_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd131f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs_max = 0\n",
    "for each in questions:\n",
    "    qs_max = max(qs_max,len(each))\n",
    "qs_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08afe05",
   "metadata": {},
   "source": [
    "# Answers modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1854d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = []\n",
    "for ans in answers:\n",
    "    seq = Sent2Seq(ans)\n",
    "    padding(seq,ans_max)\n",
    "    ANS.append(np.array(seq))\n",
    "decoder_input_data = np.array(ANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de37e806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 377)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9dc2d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  48,  178, 1073, 1097, 2020,  304, 1410,  700,  134, 1766,  593,\n",
       "       2056,  470, 1238, 2019, 2036,   16,   47,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d16203f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  48,  178, 1073, 1097, 2020,  304, 1410,  700,  134, 1766,  593,\n",
       "       2056,  470, 1238, 2019, 2036,   16,   47,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b029baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ANS)) :\n",
    "    ANS[i] = ANS[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(ANS , maxlen=ans_max , padding='post')\n",
    "onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
    "decoder_output_data = np.array(onehot_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d071d42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 377, 2268)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5e9cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ANS\n",
    "del padded_answers\n",
    "del onehot_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "770fd524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6209e",
   "metadata": {},
   "source": [
    "# Questions Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d891dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "QS = []\n",
    "for qs in questions:\n",
    "    seq = Sent2Seq(qs)\n",
    "    padding(seq,qs_max)\n",
    "    QS.append(np.array(seq))\n",
    "encoder_input_data = np.array(QS)\n",
    "del QS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a1dfd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 130)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04539163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2205, 1097,   99,   51,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f3e9e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5133b3",
   "metadata": {},
   "source": [
    "Embedding, LSTM and Desne layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51773370",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(qs_max ,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 300 , mask_zero=True) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(300 , return_state=True)(encoder_embedding)\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(ans_max , ))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 300 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(300 , return_state=True , return_sequences=True)\n",
    "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
    "\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax) \n",
    "output = decoder_dense (decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bfd0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "543fd514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 130)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 377)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 130, 300)             680400    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 377, 300)             680400    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 300),                721200    ['embedding[0][0]']           \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 377, 300),           721200    ['embedding_1[0][0]',         \n",
      "                              (None, 300),                           'lstm[0][1]',                \n",
      "                              (None, 300)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 377, 2268)            682668    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3485868 (13.30 MB)\n",
      "Trainable params: 3485868 (13.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6d840da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 375s 7s/step - loss: 6.1771 - accuracy: 0.1411\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 318s 7s/step - loss: 5.0173 - accuracy: 0.2081\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 1483s 31s/step - loss: 4.6786 - accuracy: 0.2422\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 268s 6s/step - loss: 4.4602 - accuracy: 0.2605\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 267s 6s/step - loss: 4.2902 - accuracy: 0.2875\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 263s 5s/step - loss: 4.1103 - accuracy: 0.3109\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 272s 6s/step - loss: 3.9330 - accuracy: 0.3256\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 264s 6s/step - loss: 3.7820 - accuracy: 0.3439\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 257s 5s/step - loss: 3.6199 - accuracy: 0.3656\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 291s 6s/step - loss: 3.4802 - accuracy: 0.3883\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 274s 6s/step - loss: 3.3471 - accuracy: 0.3981\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 268s 6s/step - loss: 3.2355 - accuracy: 0.4101\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 261s 5s/step - loss: 3.1131 - accuracy: 0.4192\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 274s 6s/step - loss: 2.9908 - accuracy: 0.4291\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 278s 6s/step - loss: 2.8942 - accuracy: 0.4376\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 264s 6s/step - loss: 2.7830 - accuracy: 0.4441\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 292s 6s/step - loss: 2.6685 - accuracy: 0.4545\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 277s 6s/step - loss: 2.5669 - accuracy: 0.4670\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 273s 6s/step - loss: 2.4654 - accuracy: 0.4796\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 266s 6s/step - loss: 2.3651 - accuracy: 0.4909\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 269s 6s/step - loss: 2.2647 - accuracy: 0.5048\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 283s 6s/step - loss: 2.1664 - accuracy: 0.5193\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 272s 6s/step - loss: 2.0705 - accuracy: 0.5355\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 269s 6s/step - loss: 1.9707 - accuracy: 0.5542\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 264s 5s/step - loss: 1.8846 - accuracy: 0.5767\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 265s 6s/step - loss: 1.7993 - accuracy: 0.5911\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 281s 6s/step - loss: 1.7070 - accuracy: 0.6130\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 277s 6s/step - loss: 1.6283 - accuracy: 0.6311\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 279s 6s/step - loss: 1.5467 - accuracy: 0.6488\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 272s 6s/step - loss: 1.4703 - accuracy: 0.6679\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 287s 6s/step - loss: 1.3879 - accuracy: 0.6865\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 262s 5s/step - loss: 1.3182 - accuracy: 0.7058\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 255s 5s/step - loss: 1.2581 - accuracy: 0.7178\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 280s 6s/step - loss: 1.1832 - accuracy: 0.7351\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 259s 5s/step - loss: 1.1279 - accuracy: 0.7509\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 268s 6s/step - loss: 1.0717 - accuracy: 0.7632\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 261s 5s/step - loss: 1.0167 - accuracy: 0.7798\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 263s 5s/step - loss: 0.9640 - accuracy: 0.7904\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 256s 5s/step - loss: 0.9112 - accuracy: 0.8026\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 273s 6s/step - loss: 0.8687 - accuracy: 0.8132\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 271s 6s/step - loss: 0.8219 - accuracy: 0.8229\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 265s 5s/step - loss: 0.7782 - accuracy: 0.8325\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 284s 6s/step - loss: 0.7520 - accuracy: 0.8369\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 269s 6s/step - loss: 0.7129 - accuracy: 0.8517\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 265s 6s/step - loss: 0.6749 - accuracy: 0.8578\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 264s 5s/step - loss: 0.6415 - accuracy: 0.8672\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 256s 5s/step - loss: 0.6085 - accuracy: 0.8757\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 280s 6s/step - loss: 0.5816 - accuracy: 0.8810\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 263s 6s/step - loss: 0.5577 - accuracy: 0.8845\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 268s 6s/step - loss: 0.5315 - accuracy: 0.8912\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 258s 5s/step - loss: 0.5021 - accuracy: 0.8980\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 272s 6s/step - loss: 0.4870 - accuracy: 0.9004\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 280s 6s/step - loss: 0.4717 - accuracy: 0.9000\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 259s 5s/step - loss: 0.4469 - accuracy: 0.9078\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 273s 6s/step - loss: 0.4192 - accuracy: 0.9133\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 282s 6s/step - loss: 0.3994 - accuracy: 0.9182\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 270s 6s/step - loss: 0.3811 - accuracy: 0.9234\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 266s 6s/step - loss: 0.3646 - accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 266s 6s/step - loss: 0.3492 - accuracy: 0.9280\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 258s 5s/step - loss: 0.3326 - accuracy: 0.9319\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 254s 5s/step - loss: 0.3209 - accuracy: 0.9344\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 264s 6s/step - loss: 0.3089 - accuracy: 0.9355\n",
      "Epoch 63/100\n",
      "48/48 [==============================] - 252s 5s/step - loss: 0.2983 - accuracy: 0.9365\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 271s 6s/step - loss: 0.2855 - accuracy: 0.9381\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 257s 5s/step - loss: 0.2763 - accuracy: 0.9384\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 267s 6s/step - loss: 0.2681 - accuracy: 0.9414\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 270s 6s/step - loss: 0.2566 - accuracy: 0.9441\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 273s 6s/step - loss: 0.2485 - accuracy: 0.9428\n",
      "Epoch 69/100\n",
      "48/48 [==============================] - 263s 5s/step - loss: 0.2391 - accuracy: 0.9432\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 272s 6s/step - loss: 0.2317 - accuracy: 0.9459\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 265s 5s/step - loss: 0.2243 - accuracy: 0.9465\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 261s 5s/step - loss: 0.2169 - accuracy: 0.9463\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 272s 6s/step - loss: 0.2105 - accuracy: 0.9476\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 261s 5s/step - loss: 0.2052 - accuracy: 0.9477\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 264s 5s/step - loss: 0.1987 - accuracy: 0.9501\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 263s 5s/step - loss: 0.1928 - accuracy: 0.9504\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 273s 6s/step - loss: 0.1907 - accuracy: 0.9499\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 274s 6s/step - loss: 0.1844 - accuracy: 0.9508\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 257s 5s/step - loss: 0.1820 - accuracy: 0.9503\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 263s 5s/step - loss: 0.1778 - accuracy: 0.9513\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 265s 5s/step - loss: 0.1741 - accuracy: 0.9510\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 267s 6s/step - loss: 0.1702 - accuracy: 0.9504\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 264s 6s/step - loss: 0.1647 - accuracy: 0.9504\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 272s 6s/step - loss: 0.1610 - accuracy: 0.9518\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 275s 6s/step - loss: 0.1615 - accuracy: 0.9518\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 267s 6s/step - loss: 0.1573 - accuracy: 0.9506\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 274s 6s/step - loss: 0.1540 - accuracy: 0.9518\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 263s 6s/step - loss: 0.1499 - accuracy: 0.9529\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 277s 6s/step - loss: 0.1486 - accuracy: 0.9523\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 269s 6s/step - loss: 0.1473 - accuracy: 0.9514\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 271s 6s/step - loss: 0.1439 - accuracy: 0.9513\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 269s 6s/step - loss: 0.1416 - accuracy: 0.9520\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 270s 6s/step - loss: 0.1402 - accuracy: 0.9530\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 276s 6s/step - loss: 0.1397 - accuracy: 0.9539\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 244s 5s/step - loss: 0.1381 - accuracy: 0.9522\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 260s 5s/step - loss: 0.1359 - accuracy: 0.9525\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 266s 6s/step - loss: 0.1340 - accuracy: 0.9525\n",
      "Epoch 98/100\n",
      "48/48 [==============================] - 273s 6s/step - loss: 0.1363 - accuracy: 0.9506\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 266s 6s/step - loss: 0.1324 - accuracy: 0.9529\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 257s 5s/step - loss: 0.1311 - accuracy: 0.9507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23674825a80>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=16, epochs=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1af0127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('BaseModel2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d240b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(300 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(300 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6a205e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model, dec_model = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6b77ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "enc_model.save('Encoder2.h5')\n",
    "dec_model.save('Decoder2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec3d7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 130)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 130, 300)          680400    \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 300),             721200    \n",
      "                              (None, 300),                       \n",
      "                              (None, 300)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1401600 (5.35 MB)\n",
      "Trainable params: 1401600 (5.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafcc59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 377)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 377, 300)             680400    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 300)]                0         []                            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 377, 300),           721200    ['embedding_1[0][0]',         \n",
      "                              (None, 300),                           'input_7[0][0]',             \n",
      "                              (None, 300)]                           'input_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 377, 2268)            682668    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2084268 (7.95 MB)\n",
      "Trainable params: 2084268 (7.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c2d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(input_sentence):\n",
    "    seq = Sent2Seq(input_sentence)\n",
    "    padding(seq,qs_max)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94747ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e08bc786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '!',\n",
       " 2: '\"',\n",
       " 3: '#',\n",
       " 4: '$',\n",
       " 5: '%',\n",
       " 6: '&',\n",
       " 7: \"'\",\n",
       " 8: '(',\n",
       " 9: '(a',\n",
       " 10: '(this',\n",
       " 11: ')',\n",
       " 12: '*',\n",
       " 13: '+',\n",
       " 14: ',',\n",
       " 15: '-',\n",
       " 16: '.',\n",
       " 17: '/',\n",
       " 18: '0',\n",
       " 19: '000',\n",
       " 20: '1',\n",
       " 21: '10',\n",
       " 22: '1946',\n",
       " 23: '1963',\n",
       " 24: '1990',\n",
       " 25: '2',\n",
       " 26: '2001',\n",
       " 27: '2002',\n",
       " 28: '20th',\n",
       " 29: '22',\n",
       " 30: '23',\n",
       " 31: '250',\n",
       " 32: '2nd',\n",
       " 33: '3',\n",
       " 34: '37th',\n",
       " 35: '4',\n",
       " 36: '42',\n",
       " 37: '5',\n",
       " 38: '6',\n",
       " 39: '7',\n",
       " 40: '8',\n",
       " 41: '9',\n",
       " 42: '9000',\n",
       " 43: '93',\n",
       " 44: ':',\n",
       " 45: ';',\n",
       " 46: '<',\n",
       " 47: '<end>',\n",
       " 48: '<start>',\n",
       " 49: '=',\n",
       " 50: '>',\n",
       " 51: '?',\n",
       " 52: '@',\n",
       " 53: '[',\n",
       " 54: '\\\\',\n",
       " 55: ']',\n",
       " 56: '^',\n",
       " 57: '_',\n",
       " 58: '_3000',\n",
       " 59: 'a',\n",
       " 60: 'abandon',\n",
       " 61: 'ability',\n",
       " 62: 'able',\n",
       " 63: 'about',\n",
       " 64: 'above',\n",
       " 65: 'absorbed',\n",
       " 66: 'accept',\n",
       " 67: 'access',\n",
       " 68: 'accomplish',\n",
       " 69: 'accuracy',\n",
       " 70: 'accused',\n",
       " 71: 'achieved',\n",
       " 72: 'achievements',\n",
       " 73: 'across',\n",
       " 74: 'act',\n",
       " 75: 'acting',\n",
       " 76: 'actions',\n",
       " 77: 'activism',\n",
       " 78: 'activity',\n",
       " 79: 'actually',\n",
       " 80: 'adage',\n",
       " 81: 'adapt',\n",
       " 82: 'addict',\n",
       " 83: 'addition',\n",
       " 84: 'administration',\n",
       " 85: 'admonition',\n",
       " 86: 'adorned',\n",
       " 87: 'advanced',\n",
       " 88: 'advancements',\n",
       " 89: 'advancing',\n",
       " 90: 'adventure',\n",
       " 91: 'adventurers',\n",
       " 92: 'advocating',\n",
       " 93: 'afraid',\n",
       " 94: 'after',\n",
       " 95: 'again',\n",
       " 96: 'age',\n",
       " 97: 'agent',\n",
       " 98: 'agree',\n",
       " 99: 'ai',\n",
       " 100: 'albert',\n",
       " 101: 'alcoholic',\n",
       " 102: 'algorithmic',\n",
       " 103: 'algorithmique',\n",
       " 104: 'alice',\n",
       " 105: 'alien',\n",
       " 106: 'alive',\n",
       " 107: 'all',\n",
       " 108: 'allegations',\n",
       " 109: 'alleged',\n",
       " 110: 'allegory',\n",
       " 111: 'allocation',\n",
       " 112: 'allow',\n",
       " 113: 'allowed',\n",
       " 114: 'allowing',\n",
       " 115: 'alls',\n",
       " 116: 'allure',\n",
       " 117: 'alone',\n",
       " 118: 'along',\n",
       " 119: 'also',\n",
       " 120: 'always',\n",
       " 121: 'am',\n",
       " 122: 'ambigous',\n",
       " 123: 'ambition',\n",
       " 124: 'amendemnt',\n",
       " 125: 'amendment',\n",
       " 126: 'american',\n",
       " 127: 'among',\n",
       " 128: 'amounted',\n",
       " 129: 'amounts',\n",
       " 130: 'ams',\n",
       " 131: 'amused',\n",
       " 132: 'an',\n",
       " 133: 'ancient',\n",
       " 134: 'and',\n",
       " 135: 'andes',\n",
       " 136: 'andrew',\n",
       " 137: 'android',\n",
       " 138: 'androids',\n",
       " 139: 'andromeda',\n",
       " 140: 'anger',\n",
       " 141: 'angry',\n",
       " 142: 'animals',\n",
       " 143: 'announces',\n",
       " 144: 'anomalies',\n",
       " 145: 'another',\n",
       " 146: 'answering',\n",
       " 147: 'ant',\n",
       " 148: 'anthem',\n",
       " 149: 'anthony',\n",
       " 150: 'anton',\n",
       " 151: 'any',\n",
       " 152: 'anybody',\n",
       " 153: 'anymore',\n",
       " 154: 'anyone',\n",
       " 155: 'anything',\n",
       " 156: 'aphrodite',\n",
       " 157: 'appearance',\n",
       " 158: 'appears',\n",
       " 159: 'apple',\n",
       " 160: 'applied',\n",
       " 161: 'archaeological',\n",
       " 162: 'archaeologist',\n",
       " 163: 'archaeology',\n",
       " 164: 'architectural',\n",
       " 165: 'architecture',\n",
       " 166: 'archive',\n",
       " 167: 'are',\n",
       " 168: 'argue',\n",
       " 169: 'arid',\n",
       " 170: 'arms',\n",
       " 171: 'arrogance',\n",
       " 172: 'arrogant',\n",
       " 173: 'art',\n",
       " 174: 'arthur',\n",
       " 175: 'article',\n",
       " 176: 'artifact',\n",
       " 177: 'artifacts',\n",
       " 178: 'artificial',\n",
       " 179: 'as',\n",
       " 180: 'ashamed',\n",
       " 181: 'asimov',\n",
       " 182: 'ask',\n",
       " 183: 'asked',\n",
       " 184: 'asking',\n",
       " 185: 'aspirations',\n",
       " 186: 'ass',\n",
       " 187: 'assassinated',\n",
       " 188: 'assistant',\n",
       " 189: 'assisted',\n",
       " 190: 'associated',\n",
       " 191: 'assuming',\n",
       " 192: 'astronomer',\n",
       " 193: 'astute',\n",
       " 194: 'at',\n",
       " 195: 'ate',\n",
       " 196: 'atmosphere',\n",
       " 197: 'attached',\n",
       " 198: 'attack',\n",
       " 199: 'attempts',\n",
       " 200: 'attention',\n",
       " 201: 'attracted',\n",
       " 202: 'attracting',\n",
       " 203: 'attracts',\n",
       " 204: 'author',\n",
       " 205: 'authority',\n",
       " 206: 'automobile',\n",
       " 207: 'average',\n",
       " 208: 'avogadro',\n",
       " 209: 'avoided',\n",
       " 210: 'avoiding',\n",
       " 211: 'await',\n",
       " 212: 'away',\n",
       " 213: 'awesome',\n",
       " 214: 'awfully',\n",
       " 215: 'axis',\n",
       " 216: 'ba',\n",
       " 217: 'babbage',\n",
       " 218: 'babe',\n",
       " 219: 'baby',\n",
       " 220: 'back',\n",
       " 221: 'backed',\n",
       " 222: 'background',\n",
       " 223: 'backgrounds',\n",
       " 224: 'bacteria',\n",
       " 225: 'bacteriology',\n",
       " 226: 'bad',\n",
       " 227: 'baggins',\n",
       " 228: 'ball',\n",
       " 229: 'band',\n",
       " 230: 'bank',\n",
       " 231: 'banned',\n",
       " 232: 'bar',\n",
       " 233: 'barca',\n",
       " 234: 'barcelona',\n",
       " 235: 'barefoot',\n",
       " 236: 'baseball',\n",
       " 237: 'based',\n",
       " 238: 'bases',\n",
       " 239: 'basic',\n",
       " 240: 'basketbal',\n",
       " 241: 'basketball',\n",
       " 242: 'bat',\n",
       " 243: 'bathe',\n",
       " 244: 'bathroom',\n",
       " 245: 'battle',\n",
       " 246: 'be',\n",
       " 247: 'beacon',\n",
       " 248: 'beast',\n",
       " 249: 'beat',\n",
       " 250: 'beauty',\n",
       " 251: 'became',\n",
       " 252: 'because',\n",
       " 253: 'become',\n",
       " 254: 'becomes',\n",
       " 255: 'becoming',\n",
       " 256: 'been',\n",
       " 257: 'before',\n",
       " 258: 'began',\n",
       " 259: 'begun',\n",
       " 260: 'behave',\n",
       " 261: 'behind',\n",
       " 262: 'being',\n",
       " 263: 'beings',\n",
       " 264: 'believe',\n",
       " 265: 'believed',\n",
       " 266: 'bend',\n",
       " 267: 'best',\n",
       " 268: 'bet',\n",
       " 269: 'better',\n",
       " 270: 'between',\n",
       " 271: 'beverages',\n",
       " 272: 'beyond',\n",
       " 273: 'big',\n",
       " 274: 'bilbo',\n",
       " 275: 'binary',\n",
       " 276: 'bioinformatics',\n",
       " 277: 'biology',\n",
       " 278: 'bionic',\n",
       " 279: 'bit',\n",
       " 280: 'blade',\n",
       " 281: 'blame',\n",
       " 282: 'blend',\n",
       " 283: 'blocked',\n",
       " 284: 'board',\n",
       " 285: 'boarded',\n",
       " 286: 'body',\n",
       " 287: 'boll',\n",
       " 288: 'book',\n",
       " 289: 'books',\n",
       " 290: 'bored',\n",
       " 291: 'boredom',\n",
       " 292: 'born',\n",
       " 293: 'boss',\n",
       " 294: 'bot',\n",
       " 295: 'both',\n",
       " 296: 'bothered',\n",
       " 297: 'bots',\n",
       " 298: 'bound',\n",
       " 299: 'boyfriend',\n",
       " 300: 'bradbury',\n",
       " 301: 'braggadaccio',\n",
       " 302: 'bragging',\n",
       " 303: 'brain',\n",
       " 304: 'branch',\n",
       " 305: 'breath',\n",
       " 306: 'breathe',\n",
       " 307: 'brightest',\n",
       " 308: 'brilliant',\n",
       " 309: 'bringing',\n",
       " 310: 'britain',\n",
       " 311: 'british',\n",
       " 312: 'broad',\n",
       " 313: 'brothers',\n",
       " 314: 'buddhist',\n",
       " 315: 'bug',\n",
       " 316: 'building',\n",
       " 317: 'built',\n",
       " 318: 'burger',\n",
       " 319: 'burial',\n",
       " 320: 'burn',\n",
       " 321: 'business',\n",
       " 322: 'bustling',\n",
       " 323: 'busy',\n",
       " 324: 'but',\n",
       " 325: 'buy',\n",
       " 326: 'buying',\n",
       " 327: 'buzzed',\n",
       " 328: 'by',\n",
       " 329: 'c',\n",
       " 330: 'calculations',\n",
       " 331: 'called',\n",
       " 332: 'callused',\n",
       " 333: 'calluses',\n",
       " 334: 'came',\n",
       " 335: 'can',\n",
       " 336: 'cancer',\n",
       " 337: 'canned',\n",
       " 338: 'cannot',\n",
       " 339: 'canopy',\n",
       " 340: 'canterbury',\n",
       " 341: 'canturbury',\n",
       " 342: 'capability',\n",
       " 343: 'capable',\n",
       " 344: 'capacity',\n",
       " 345: 'capitalism',\n",
       " 346: 'captivated',\n",
       " 347: 'carbon',\n",
       " 348: 'carcinogen',\n",
       " 349: 'carefully',\n",
       " 350: 'carnation',\n",
       " 351: 'carolina',\n",
       " 352: 'carrion',\n",
       " 353: 'carrying',\n",
       " 354: 'cartune',\n",
       " 355: 'carvings',\n",
       " 356: 'case',\n",
       " 357: 'casino',\n",
       " 358: 'castle',\n",
       " 359: 'cat',\n",
       " 360: 'cataclysmic',\n",
       " 361: 'cataloging',\n",
       " 362: 'catcher',\n",
       " 363: 'caulfield',\n",
       " 364: 'cause',\n",
       " 365: 'caused',\n",
       " 366: 'cave',\n",
       " 367: 'cells',\n",
       " 368: 'celtic',\n",
       " 369: 'center',\n",
       " 370: 'central',\n",
       " 371: 'centre',\n",
       " 372: 'centuries',\n",
       " 373: 'century',\n",
       " 374: 'cereal',\n",
       " 375: 'certainly',\n",
       " 376: 'challenge',\n",
       " 377: 'challenged',\n",
       " 378: 'challenges',\n",
       " 379: 'chamber',\n",
       " 380: 'changes',\n",
       " 381: 'channels',\n",
       " 382: 'character',\n",
       " 383: 'characteristics',\n",
       " 384: 'characterized',\n",
       " 385: 'charge',\n",
       " 386: 'charlatan',\n",
       " 387: 'charles',\n",
       " 388: 'chat',\n",
       " 389: 'chatterbots',\n",
       " 390: 'chatterbox',\n",
       " 391: 'chaucer',\n",
       " 392: 'cheat',\n",
       " 393: 'cheating',\n",
       " 394: 'check',\n",
       " 395: 'checks',\n",
       " 396: 'cheetah',\n",
       " 397: 'chemicals',\n",
       " 398: 'chemistry',\n",
       " 399: 'chicken',\n",
       " 400: 'chiefly',\n",
       " 401: 'child',\n",
       " 402: 'children',\n",
       " 403: 'chilly',\n",
       " 404: 'china',\n",
       " 405: 'chip',\n",
       " 406: 'choose',\n",
       " 407: 'circuit',\n",
       " 408: 'cited',\n",
       " 409: 'cities',\n",
       " 410: 'city',\n",
       " 411: 'civil',\n",
       " 412: 'civilization',\n",
       " 413: 'clark',\n",
       " 414: 'class',\n",
       " 415: 'classless',\n",
       " 416: 'cliffs',\n",
       " 417: 'clinical',\n",
       " 418: 'clock',\n",
       " 419: 'clone',\n",
       " 420: 'clones',\n",
       " 421: 'cloning',\n",
       " 422: 'close',\n",
       " 423: 'closely',\n",
       " 424: 'closer',\n",
       " 425: 'club',\n",
       " 426: 'clues',\n",
       " 427: 'code',\n",
       " 428: 'cold',\n",
       " 429: 'collaboration',\n",
       " 430: 'collection',\n",
       " 431: 'collective',\n",
       " 432: 'colossal',\n",
       " 433: 'combined',\n",
       " 434: 'come',\n",
       " 435: 'comedy',\n",
       " 436: 'comes',\n",
       " 437: 'comic',\n",
       " 438: 'commander',\n",
       " 439: 'common',\n",
       " 440: 'commonly',\n",
       " 441: 'communicate',\n",
       " 442: 'communism',\n",
       " 443: 'communist',\n",
       " 444: 'community',\n",
       " 445: 'company',\n",
       " 446: 'compared',\n",
       " 447: 'competing',\n",
       " 448: 'competition',\n",
       " 449: 'competitions',\n",
       " 450: 'competitive',\n",
       " 451: 'complex',\n",
       " 452: 'compliment',\n",
       " 453: 'component',\n",
       " 454: 'components',\n",
       " 455: 'computer',\n",
       " 456: 'computers',\n",
       " 457: 'concerned',\n",
       " 458: 'concerns',\n",
       " 459: 'condition',\n",
       " 460: 'conditions',\n",
       " 461: 'conflict',\n",
       " 462: 'connections',\n",
       " 463: 'conpiracy',\n",
       " 464: 'conservation',\n",
       " 465: 'considered',\n",
       " 466: 'consisting',\n",
       " 467: 'conspiracies',\n",
       " 468: 'conspiracy',\n",
       " 469: 'construct',\n",
       " 470: 'constructing',\n",
       " 471: 'consume',\n",
       " 472: 'consumption',\n",
       " 473: 'contained',\n",
       " 474: 'containing',\n",
       " 475: 'contents',\n",
       " 476: 'context',\n",
       " 477: 'contiguous',\n",
       " 478: 'continent',\n",
       " 479: 'continental',\n",
       " 480: 'continents',\n",
       " 481: 'continue',\n",
       " 482: 'continued',\n",
       " 483: 'contrary',\n",
       " 484: 'control',\n",
       " 485: 'conversation',\n",
       " 486: 'conversations',\n",
       " 487: 'conversions',\n",
       " 488: 'cool',\n",
       " 489: 'coordinates',\n",
       " 490: 'coordination',\n",
       " 491: 'copied',\n",
       " 492: 'copies',\n",
       " 493: 'copper',\n",
       " 494: 'copy',\n",
       " 495: 'copying',\n",
       " 496: 'corporeal',\n",
       " 497: 'corpus',\n",
       " 498: 'corrupt',\n",
       " 499: 'could',\n",
       " 500: 'counseling',\n",
       " 501: 'count',\n",
       " 502: 'counterproductive',\n",
       " 503: 'country',\n",
       " 504: 'counts',\n",
       " 505: 'couple',\n",
       " 506: 'courage',\n",
       " 507: 'course',\n",
       " 508: 'covered',\n",
       " 509: 'cow',\n",
       " 510: 'coward',\n",
       " 511: 'cpu',\n",
       " 512: 'cpus',\n",
       " 513: 'craft',\n",
       " 514: 'cramped',\n",
       " 515: 'crashes',\n",
       " 516: 'crazy',\n",
       " 517: 'creaked',\n",
       " 518: 'create',\n",
       " 519: 'created',\n",
       " 520: 'creating',\n",
       " 521: 'creativity',\n",
       " 522: 'creatures',\n",
       " 523: 'cricket',\n",
       " 524: 'critical',\n",
       " 525: 'cross',\n",
       " 526: 'cruel',\n",
       " 527: 'crystalline',\n",
       " 528: 'crystallography',\n",
       " 529: 'crystals',\n",
       " 530: 'cultural',\n",
       " 531: 'cultures',\n",
       " 532: 'curiosity',\n",
       " 533: 'curious',\n",
       " 534: 'currency',\n",
       " 535: 'current',\n",
       " 536: 'cyberpunk',\n",
       " 537: 'cytology',\n",
       " 538: 'd',\n",
       " 539: 'damage',\n",
       " 540: 'damaged',\n",
       " 541: 'dance',\n",
       " 542: 'dare',\n",
       " 543: 'dared',\n",
       " 544: 'data',\n",
       " 545: 'database',\n",
       " 546: 'dawn',\n",
       " 547: 'day',\n",
       " 548: 'days',\n",
       " 549: 'dead',\n",
       " 550: 'dealing',\n",
       " 551: 'deals',\n",
       " 552: 'dear',\n",
       " 553: 'deathless',\n",
       " 554: 'deceitful',\n",
       " 555: 'deceiving',\n",
       " 556: 'decided',\n",
       " 557: 'decipher',\n",
       " 558: 'deciphered',\n",
       " 559: 'deciphering',\n",
       " 560: 'dedicate',\n",
       " 561: 'deep',\n",
       " 562: 'deeper',\n",
       " 563: 'defining',\n",
       " 564: 'definitely',\n",
       " 565: 'definition',\n",
       " 566: 'deleted',\n",
       " 567: 'demand',\n",
       " 568: 'deniably',\n",
       " 569: 'dense',\n",
       " 570: 'dental',\n",
       " 571: 'dentist',\n",
       " 572: 'department',\n",
       " 573: 'depending',\n",
       " 574: 'depends',\n",
       " 575: 'depicting',\n",
       " 576: 'deployed',\n",
       " 577: 'depths',\n",
       " 578: 'deranged',\n",
       " 579: 'derangement',\n",
       " 580: 'describe',\n",
       " 581: 'descriptive',\n",
       " 582: 'deserts',\n",
       " 583: 'designed',\n",
       " 584: 'desks',\n",
       " 585: 'despite',\n",
       " 586: 'detailing',\n",
       " 587: 'detect',\n",
       " 588: 'determination',\n",
       " 589: 'determine',\n",
       " 590: 'developed',\n",
       " 591: 'device',\n",
       " 592: 'devices',\n",
       " 593: 'devoted',\n",
       " 594: 'diagnosed',\n",
       " 595: 'diagnosis',\n",
       " 596: 'diamond',\n",
       " 597: 'dick',\n",
       " 598: 'dictionary',\n",
       " 599: 'did',\n",
       " 600: 'die',\n",
       " 601: 'diet',\n",
       " 602: 'difference',\n",
       " 603: 'different',\n",
       " 604: 'differentiated',\n",
       " 605: 'difficult',\n",
       " 606: 'digital',\n",
       " 607: 'digits',\n",
       " 608: 'ding',\n",
       " 609: 'dioxide',\n",
       " 610: 'dire',\n",
       " 611: 'direction',\n",
       " 612: 'dirty',\n",
       " 613: 'discover',\n",
       " 614: 'discovered',\n",
       " 615: 'discoveries',\n",
       " 616: 'discovery',\n",
       " 617: 'discredited',\n",
       " 618: 'discrete',\n",
       " 619: 'disease',\n",
       " 620: 'diseases',\n",
       " 621: 'disgusting',\n",
       " 622: 'dishonest',\n",
       " 623: 'disk',\n",
       " 624: 'distance',\n",
       " 625: 'distribution',\n",
       " 626: 'district',\n",
       " 627: 'diverse',\n",
       " 628: 'division',\n",
       " 629: 'do',\n",
       " 630: 'documentation',\n",
       " 631: 'documented',\n",
       " 632: 'does',\n",
       " 633: 'dog',\n",
       " 634: 'doing',\n",
       " 635: 'dollar',\n",
       " 636: 'dolphins',\n",
       " 637: 'done',\n",
       " 638: 'dont',\n",
       " 639: 'dostoyevsky',\n",
       " 640: 'doubt',\n",
       " 641: 'down',\n",
       " 642: 'drawing',\n",
       " 643: 'dream',\n",
       " 644: 'dreams',\n",
       " 645: 'drink',\n",
       " 646: 'driven',\n",
       " 647: 'drop',\n",
       " 648: 'drops',\n",
       " 649: 'drunk',\n",
       " 650: 'dull',\n",
       " 651: 'dumb',\n",
       " 652: 'dune',\n",
       " 653: 'dusty',\n",
       " 654: 'dynamics',\n",
       " 655: 'each',\n",
       " 656: 'earn',\n",
       " 657: 'earth',\n",
       " 658: 'easily',\n",
       " 659: 'eat',\n",
       " 660: 'echolocation',\n",
       " 661: 'eclectic',\n",
       " 662: 'economic',\n",
       " 663: 'economics',\n",
       " 664: 'edison',\n",
       " 665: 'edition',\n",
       " 666: 'edwin',\n",
       " 667: 'effectively',\n",
       " 668: 'eggs',\n",
       " 669: 'ego',\n",
       " 670: 'einstein',\n",
       " 671: 'either',\n",
       " 672: 'electric',\n",
       " 673: 'electricity',\n",
       " 674: 'electronic',\n",
       " 675: 'elena',\n",
       " 676: 'eleven',\n",
       " 677: 'eliza',\n",
       " 678: 'elusive',\n",
       " 679: 'embarassed',\n",
       " 680: 'embarassment',\n",
       " 681: 'embark',\n",
       " 682: 'embarrassed',\n",
       " 683: 'emote',\n",
       " 684: 'emotion',\n",
       " 685: 'emotional',\n",
       " 686: 'emotions',\n",
       " 687: 'employed',\n",
       " 688: 'emulate',\n",
       " 689: 'emulating',\n",
       " 690: 'encoded',\n",
       " 691: 'encountered',\n",
       " 692: 'encountering',\n",
       " 693: 'end',\n",
       " 694: 'endangers',\n",
       " 695: 'endless',\n",
       " 696: 'endurance',\n",
       " 697: 'energy',\n",
       " 698: 'engages',\n",
       " 699: 'engine',\n",
       " 700: 'engineering',\n",
       " 701: 'eniac',\n",
       " 702: 'enigmatic',\n",
       " 703: 'enjoy',\n",
       " 704: 'enough',\n",
       " 705: 'entertainment',\n",
       " 706: 'enthralled',\n",
       " 707: 'entire',\n",
       " 708: 'entities',\n",
       " 709: 'entity',\n",
       " 710: 'entrance',\n",
       " 711: 'entropy',\n",
       " 712: 'enviornmental',\n",
       " 713: 'environment',\n",
       " 714: 'eras',\n",
       " 715: 'erased',\n",
       " 716: 'erred',\n",
       " 717: 'error',\n",
       " 718: 'eskimos',\n",
       " 719: 'especially',\n",
       " 720: 'essence',\n",
       " 721: 'established',\n",
       " 722: 'etc',\n",
       " 723: 'ethereal',\n",
       " 724: 'europe',\n",
       " 725: 'even',\n",
       " 726: 'event',\n",
       " 727: 'events',\n",
       " 728: 'eventually',\n",
       " 729: 'ever',\n",
       " 730: 'everthing',\n",
       " 731: 'every',\n",
       " 732: 'everyday',\n",
       " 733: 'everything',\n",
       " 734: 'everywhere',\n",
       " 735: 'evidence',\n",
       " 736: 'exactly',\n",
       " 737: 'examined',\n",
       " 738: 'example',\n",
       " 739: 'excellent',\n",
       " 740: 'except',\n",
       " 741: 'exception',\n",
       " 742: 'exchange',\n",
       " 743: 'excited',\n",
       " 744: 'execute',\n",
       " 745: 'exeptions',\n",
       " 746: 'exhaust',\n",
       " 747: 'existed',\n",
       " 748: 'existence',\n",
       " 749: 'expect',\n",
       " 750: 'expecting',\n",
       " 751: 'expeditions',\n",
       " 752: 'experience',\n",
       " 753: 'experiencing',\n",
       " 754: 'experimental',\n",
       " 755: 'explain',\n",
       " 756: 'exploration',\n",
       " 757: 'explorations',\n",
       " 758: 'explore',\n",
       " 759: 'explored',\n",
       " 760: 'explorer',\n",
       " 761: 'explorers',\n",
       " 762: 'express',\n",
       " 763: 'expressed',\n",
       " 764: 'eyes',\n",
       " 765: 'f',\n",
       " 766: 'face',\n",
       " 767: 'faced',\n",
       " 768: 'factories',\n",
       " 769: 'failed',\n",
       " 770: 'fairly',\n",
       " 771: 'fakie',\n",
       " 772: 'fall',\n",
       " 773: 'famous',\n",
       " 774: 'fan',\n",
       " 775: 'fancy',\n",
       " 776: 'far',\n",
       " 777: 'fast',\n",
       " 778: 'fateful',\n",
       " 779: 'father',\n",
       " 780: 'faults',\n",
       " 781: 'favorite',\n",
       " 782: 'favourite',\n",
       " 783: 'fear',\n",
       " 784: 'feasible',\n",
       " 785: 'feats',\n",
       " 786: 'feel',\n",
       " 787: 'feeling',\n",
       " 788: 'feelings',\n",
       " 789: 'feels',\n",
       " 790: 'feet',\n",
       " 791: 'fell',\n",
       " 792: 'felt',\n",
       " 793: 'female',\n",
       " 794: 'fever',\n",
       " 795: 'few',\n",
       " 796: 'fi',\n",
       " 797: 'fiction',\n",
       " 798: 'fictional',\n",
       " 799: 'field',\n",
       " 800: 'fight',\n",
       " 801: 'fighting',\n",
       " 802: 'files',\n",
       " 803: 'filesystem',\n",
       " 804: 'fill',\n",
       " 805: 'filled',\n",
       " 806: 'film',\n",
       " 807: 'filtering',\n",
       " 808: 'final',\n",
       " 809: 'finals',\n",
       " 810: 'finance',\n",
       " 811: 'find',\n",
       " 812: 'findings',\n",
       " 813: 'fine',\n",
       " 814: 'finished',\n",
       " 815: 'fire',\n",
       " 816: 'firewall',\n",
       " 817: 'firewalls',\n",
       " 818: 'first',\n",
       " 819: 'fishes',\n",
       " 820: 'flakes',\n",
       " 821: 'flawless',\n",
       " 822: 'flaws',\n",
       " 823: 'flesh',\n",
       " 824: 'fluent',\n",
       " 825: 'follows',\n",
       " 826: 'fond',\n",
       " 827: 'food',\n",
       " 828: 'fool',\n",
       " 829: 'football',\n",
       " 830: 'for',\n",
       " 831: 'force',\n",
       " 832: 'forced',\n",
       " 833: 'forests',\n",
       " 834: 'forever',\n",
       " 835: 'forget',\n",
       " 836: 'forgetting',\n",
       " 837: 'form',\n",
       " 838: 'formed',\n",
       " 839: 'formidable',\n",
       " 840: 'forming',\n",
       " 841: 'forms',\n",
       " 842: 'found',\n",
       " 843: 'foundation',\n",
       " 844: 'four',\n",
       " 845: 'fragile',\n",
       " 846: 'frail',\n",
       " 847: 'frank',\n",
       " 848: 'frankenstein',\n",
       " 849: 'free',\n",
       " 850: 'frenetic',\n",
       " 851: 'frequency',\n",
       " 852: 'friend',\n",
       " 853: 'friends',\n",
       " 854: 'frighten',\n",
       " 855: 'from',\n",
       " 856: 'frosted',\n",
       " 857: 'frustrated',\n",
       " 858: 'frustration',\n",
       " 859: 'fully',\n",
       " 860: 'fun',\n",
       " 861: 'function',\n",
       " 862: 'functionally',\n",
       " 863: 'functions',\n",
       " 864: 'funds',\n",
       " 865: 'further',\n",
       " 866: 'future',\n",
       " 867: 'fyodor',\n",
       " 868: 'gained',\n",
       " 869: 'galaxy',\n",
       " 870: 'game',\n",
       " 871: 'gandhi',\n",
       " 872: 'gardens',\n",
       " 873: 'gate',\n",
       " 874: 'gathering',\n",
       " 875: 'gene',\n",
       " 876: 'general',\n",
       " 877: 'generally',\n",
       " 878: 'generations',\n",
       " 879: 'geoffrey',\n",
       " 880: 'george',\n",
       " 881: 'get',\n",
       " 882: 'giant',\n",
       " 883: 'gibson',\n",
       " 884: 'give',\n",
       " 885: 'given',\n",
       " 886: 'gives',\n",
       " 887: 'glad',\n",
       " 888: 'global',\n",
       " 889: 'glove',\n",
       " 890: 'glowing',\n",
       " 891: 'go',\n",
       " 892: 'goal',\n",
       " 893: 'goals',\n",
       " 894: 'goats',\n",
       " 895: 'god',\n",
       " 896: 'goddess',\n",
       " 897: 'godzilla',\n",
       " 898: 'goes',\n",
       " 899: 'going',\n",
       " 900: 'gold',\n",
       " 901: 'good',\n",
       " 902: 'gossip',\n",
       " 903: 'gossips',\n",
       " 904: 'got',\n",
       " 905: 'governed',\n",
       " 906: 'governing',\n",
       " 907: 'government',\n",
       " 908: 'governments',\n",
       " 909: 'governor',\n",
       " 910: 'grammatical',\n",
       " 911: 'grand',\n",
       " 912: 'gravitation',\n",
       " 913: 'great',\n",
       " 914: 'greater',\n",
       " 915: 'greatest',\n",
       " 916: 'greek',\n",
       " 917: 'greenfield',\n",
       " 918: 'greenpeace',\n",
       " 919: 'greeted',\n",
       " 920: 'greetings',\n",
       " 921: 'gregory',\n",
       " 922: 'grew',\n",
       " 923: 'group',\n",
       " 924: 'grudges',\n",
       " 925: 'guarded',\n",
       " 926: 'guardians',\n",
       " 927: 'guide',\n",
       " 928: 'guiltier',\n",
       " 929: 'guilty',\n",
       " 930: 'guns',\n",
       " 931: 'gutenberg',\n",
       " 932: 'guy',\n",
       " 933: 'gyroscope',\n",
       " 934: 'h',\n",
       " 935: 'h2o',\n",
       " 936: 'habib',\n",
       " 937: 'had',\n",
       " 938: 'hal',\n",
       " 939: 'hal9000',\n",
       " 940: 'halitosis',\n",
       " 941: 'hamburger',\n",
       " 942: 'hand',\n",
       " 943: 'hands',\n",
       " 944: 'hans',\n",
       " 945: 'happen',\n",
       " 946: 'happily',\n",
       " 947: 'happiness',\n",
       " 948: 'happy',\n",
       " 949: 'hard',\n",
       " 950: 'harder',\n",
       " 951: 'hardships',\n",
       " 952: 'hardware',\n",
       " 953: 'hare',\n",
       " 954: 'has',\n",
       " 955: 'hat',\n",
       " 956: 'hate',\n",
       " 957: 'have',\n",
       " 958: 'haven',\n",
       " 959: 'he',\n",
       " 960: 'health',\n",
       " 961: 'hear',\n",
       " 962: 'heard',\n",
       " 963: 'heart',\n",
       " 964: 'heat',\n",
       " 965: 'held',\n",
       " 966: 'hello',\n",
       " 967: 'help',\n",
       " 968: 'her',\n",
       " 969: 'herbert',\n",
       " 970: 'herd',\n",
       " 971: 'here',\n",
       " 972: 'heritage',\n",
       " 973: 'herman',\n",
       " 974: 'heroes',\n",
       " 975: 'herself',\n",
       " 976: 'heuristic',\n",
       " 977: 'heuristique',\n",
       " 978: 'hexed',\n",
       " 979: 'hi',\n",
       " 980: 'hidden',\n",
       " 981: 'hide',\n",
       " 982: 'high',\n",
       " 983: 'highly',\n",
       " 984: 'hills',\n",
       " 985: 'him',\n",
       " 986: 'his',\n",
       " 987: 'historical',\n",
       " 988: 'history',\n",
       " 989: 'hobbit',\n",
       " 990: 'hobby',\n",
       " 991: 'hold',\n",
       " 992: 'holden',\n",
       " 993: 'hollywood',\n",
       " 994: 'holsteins',\n",
       " 995: 'homer',\n",
       " 996: 'honest',\n",
       " 997: 'honor',\n",
       " 998: 'hoops',\n",
       " 999: 'hope',\n",
       " 1000: 'hopeless',\n",
       " ...}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = {i:w for w,i in zip(vocab.keys(),vocab.values())}\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6baa18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['You can not move .', 'You sound like Data !', 'Stupid !', 'you are idiot .', 'i am going to die ?','who are you ?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644457b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2259,  335, 1392, 1332,   16,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [preprocess_input(tests[0])]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf776756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "states_values = enc_model.predict(np.array(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d1db6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.5839774e-03,  6.7818805e-04, -4.0749136e-02,  1.4696525e-03,\n",
       "         -2.2133898e-05,  9.8960382e-01,  1.2886683e-02,  1.8648434e-02,\n",
       "          3.5040519e-01,  5.9976453e-01, -9.9838877e-01, -2.7573099e-06,\n",
       "          4.1376578e-04, -2.5494923e-03, -1.0890899e-03, -2.4165046e-09,\n",
       "         -5.1991985e-04,  8.4895990e-04,  9.9958587e-01, -9.9518001e-01,\n",
       "          6.9993403e-06,  1.7146427e-04, -3.2943513e-04,  1.8610771e-01,\n",
       "          2.1843273e-02, -2.6579376e-02,  9.8051584e-01,  7.0314974e-01,\n",
       "         -8.9817804e-01,  2.4182510e-02,  8.3815306e-01,  9.9650067e-01,\n",
       "         -9.9569017e-01,  5.1534700e-01, -9.9481350e-01, -9.9158329e-01,\n",
       "          3.2736029e-02, -5.9846163e-01,  1.4589143e-03,  2.0138773e-03,\n",
       "         -2.8750014e-03,  1.2582901e-01,  9.9199003e-01,  3.9880739e-05,\n",
       "          5.3726286e-02, -8.3559105e-04,  1.0580263e-07, -7.5216102e-03,\n",
       "         -5.7110947e-02, -1.1290421e-02, -1.0867003e-02,  9.9771518e-01,\n",
       "         -9.4756000e-02, -8.1493074e-01,  4.4709051e-01, -9.9555516e-01,\n",
       "          1.1379918e-05, -7.2928315e-01, -9.7892183e-01, -1.1739225e-01,\n",
       "         -2.3395622e-05, -1.6475065e-05,  9.9847537e-01, -9.8595560e-01,\n",
       "          9.7759622e-01, -9.9811977e-01,  2.3734188e-01,  9.9419338e-01,\n",
       "         -6.5373458e-02, -9.3676472e-01, -3.3640154e-04,  9.6248375e-05,\n",
       "          9.9919355e-01, -9.8241621e-01,  2.1385701e-01, -5.9883517e-01,\n",
       "          9.6956789e-01, -8.0478466e-01,  6.6043953e-03, -2.4792635e-07,\n",
       "         -2.3085189e-01, -4.3552399e-01,  3.2658848e-01, -2.1949664e-03,\n",
       "          3.0807975e-01, -1.7554423e-01,  9.8932445e-01,  2.5898950e-02,\n",
       "          3.7228301e-02,  2.2080943e-01,  7.4581057e-01,  1.6245234e-05,\n",
       "         -9.9990427e-01, -9.9989682e-01, -9.5825601e-01, -3.2368445e-01,\n",
       "         -9.9683946e-01,  9.9797553e-01,  1.9605577e-01,  7.6350427e-01,\n",
       "         -9.9474114e-01, -8.5482025e-01,  9.9855447e-01, -1.1314559e-02,\n",
       "         -7.8995363e-06, -9.9734288e-01,  3.2154486e-01, -3.3242803e-03,\n",
       "         -9.9927223e-01, -8.0460662e-01,  2.8942620e-06,  2.8269258e-01,\n",
       "         -8.1166983e-01,  5.3106914e-03, -7.5800563e-05,  5.0280273e-01,\n",
       "         -4.2451870e-02,  9.1309464e-01,  7.9049254e-03, -7.9547799e-01,\n",
       "         -2.0660660e-07, -6.4779748e-03,  2.0017360e-06,  9.9966371e-01,\n",
       "          7.3271714e-02,  8.7869819e-03, -8.7537217e-01,  4.6499044e-02,\n",
       "          8.6496294e-01,  1.8235644e-02, -2.1512144e-06, -9.9972159e-01,\n",
       "          2.1253624e-07,  9.9402314e-01, -8.4814695e-07,  9.6940443e-05,\n",
       "          7.7811144e-08, -7.3609447e-01, -6.8780261e-01, -3.5952032e-04,\n",
       "          4.5479000e-01,  9.7583091e-01,  5.7336655e-07, -8.1168008e-01,\n",
       "         -9.8705375e-01, -1.9221886e-01, -1.6865857e-05, -7.1779776e-01,\n",
       "          9.9990523e-01, -9.4592559e-01,  1.1508869e-01,  9.4240785e-01,\n",
       "          3.8848643e-04, -9.8653096e-01, -9.9354327e-02,  3.8201071e-04,\n",
       "         -7.6015276e-01, -1.7980131e-03, -1.3967493e-02, -2.7436741e-07,\n",
       "          9.8238289e-01, -9.5086078e-05,  8.5174173e-01,  9.9971551e-01,\n",
       "          3.6973876e-01,  9.8864305e-01, -5.4327500e-01,  9.8394144e-01,\n",
       "         -1.9920975e-02, -1.0217800e-03,  9.9547577e-01,  5.2099335e-07,\n",
       "         -9.9989396e-01,  7.6682243e-04, -2.6903054e-01, -1.9254217e-01,\n",
       "         -1.3100887e-05, -2.0633353e-02, -2.2930473e-02, -1.1637716e-06,\n",
       "         -1.5992720e-07, -8.4135274e-04, -9.6354842e-01, -1.3824584e-01,\n",
       "         -3.7933648e-07,  6.3171870e-01, -6.6673056e-05,  9.9990457e-01,\n",
       "         -9.7365469e-01,  6.2611276e-01, -5.4977477e-01,  9.8333979e-01,\n",
       "          2.7749194e-03, -1.1750465e-03, -1.3499327e-04,  9.9920923e-01,\n",
       "          7.8330068e-03, -5.3288358e-01, -1.2071156e-03, -1.3907357e-03,\n",
       "          1.6513017e-01, -8.3482154e-03, -1.3622408e-03, -2.8712988e-01,\n",
       "         -9.1851747e-01, -9.3940461e-01,  2.5531301e-06,  9.6775719e-04,\n",
       "          7.7807838e-01,  1.3055472e-04, -8.5265480e-02, -9.6295077e-01,\n",
       "          7.7515429e-01,  9.9684614e-01, -1.9868377e-03, -3.8434905e-01,\n",
       "          4.8629745e-06, -8.1491715e-01,  3.1174344e-03,  2.5742394e-03,\n",
       "         -6.5000248e-01,  2.2667462e-05, -9.3370346e-07, -9.4209003e-01,\n",
       "          2.4561081e-07, -9.6418977e-02,  2.1950386e-02, -2.7876496e-01,\n",
       "          4.9747917e-01,  7.7215618e-01, -5.1925462e-01,  7.2855693e-01,\n",
       "          2.6253054e-01,  1.0470137e-03, -8.5907847e-01, -2.4829959e-07,\n",
       "         -1.3412259e-06, -9.9305654e-01,  9.9797016e-01, -9.8719883e-01,\n",
       "         -3.2763717e-01, -2.2891155e-01,  6.0573103e-07, -9.7597343e-01,\n",
       "         -9.9885780e-01, -6.1436098e-05,  7.0636019e-02,  6.6730791e-01,\n",
       "         -1.3061941e-02, -5.5774164e-01,  3.2705506e-03, -3.2108296e-02,\n",
       "          9.7613919e-01,  1.7720604e-02, -9.9182791e-01,  9.7864449e-01,\n",
       "         -9.9911582e-01,  2.1271731e-03,  2.5100573e-03,  9.9530435e-01,\n",
       "         -9.2586654e-01, -9.9878156e-01,  9.7539264e-01, -1.4589908e-02,\n",
       "         -2.6310989e-01,  9.8830807e-01, -9.2988861e-01,  1.2004998e-05,\n",
       "         -9.1425682e-06,  1.1905997e-07,  2.0510042e-02, -9.9963009e-01,\n",
       "         -9.8722577e-01, -1.1167661e-05, -7.8796911e-01,  9.6119416e-01,\n",
       "          3.5558460e-06, -3.9153456e-06,  2.1001075e-03, -9.7621483e-01,\n",
       "          9.9503703e-04,  4.9200021e-03, -7.6952560e-09, -3.5210302e-01,\n",
       "          9.3605137e-01, -2.9914039e-01,  1.7261955e-01,  7.2610009e-01,\n",
       "         -4.5540850e-03, -6.3472442e-05, -8.9757478e-01,  1.7284214e-02,\n",
       "         -9.9522364e-01, -8.5125643e-01,  3.1021654e-04,  2.8584886e-03,\n",
       "          1.5473987e-04, -1.5273169e-07, -9.9990845e-01, -9.9937093e-01]],\n",
       "       dtype=float32),\n",
       " array([[-3.1478107e+00,  4.2243981e+00, -4.0776841e-02,  2.9105556e+00,\n",
       "         -4.8432384e+00,  4.1864185e+00,  3.7385612e+00,  3.6000533e+00,\n",
       "          3.2233372e+00,  7.3316634e-01, -3.5626588e+00, -4.4518943e+00,\n",
       "          4.1390548e-04, -3.7241950e+00, -4.5375981e+00, -2.4113348e-01,\n",
       "         -1.6144269e+00,  3.0912097e+00,  4.2418785e+00, -3.0223439e+00,\n",
       "          4.8579316e+00,  1.0031450e+00, -2.5970101e+00,  1.8831283e-01,\n",
       "          1.7329544e+00, -7.8977458e-02,  4.5474730e+00,  8.7350357e-01,\n",
       "         -1.4634829e+00,  4.5918412e+00,  4.1310120e+00,  3.2201607e+00,\n",
       "         -3.0716739e+00,  4.1544247e+00, -2.9795854e+00, -2.9074960e+00,\n",
       "          1.8610327e+00, -6.9110298e-01,  3.6726844e+00,  4.5534697e+00,\n",
       "         -4.8579006e+00,  4.9990263e+00,  2.7630782e+00,  4.4927692e+00,\n",
       "          4.1584468e+00, -4.8921008e+00,  4.9056892e+00, -3.3850093e+00,\n",
       "         -8.1527734e-01, -3.8586864e+00, -2.5592856e+00,  3.3872082e+00,\n",
       "         -4.0499482e+00, -4.9970598e+00,  3.9747906e+00, -3.5631993e+00,\n",
       "          9.5449376e-01, -9.2720288e-01, -3.4810774e+00, -4.0354648e+00,\n",
       "         -4.5413828e+00, -4.7101388e+00,  4.2799935e+00, -2.4809649e+00,\n",
       "          4.4930272e+00, -3.4845119e+00,  2.4200904e-01,  3.0391695e+00,\n",
       "         -4.8921676e+00, -1.7109686e+00, -4.4338140e+00,  4.3949957e+00,\n",
       "          3.9080346e+00, -4.9869037e+00,  4.7201128e+00, -3.6258988e+00,\n",
       "          3.3047051e+00, -1.9334348e+00,  1.7061209e-02, -9.1895819e-01,\n",
       "         -4.1244364e+00, -5.3092605e-01,  4.0262280e+00, -2.1949983e-03,\n",
       "          1.2177659e+00, -7.0648080e-01,  4.3861032e+00,  4.7837186e+00,\n",
       "          4.6318946e+00,  3.6522980e+00,  9.6344751e-01,  4.3307953e+00,\n",
       "         -4.9753342e+00, -4.9853554e+00, -3.4593754e+00, -7.6917619e-01,\n",
       "         -3.2243421e+00,  3.8066542e+00,  3.6865430e+00,  4.6947546e+00,\n",
       "         -3.0135798e+00, -4.1604757e+00,  3.7615538e+00, -4.7098804e+00,\n",
       "         -4.7356858e+00, -3.4399815e+00,  4.9023371e+00, -4.6884332e+00,\n",
       "         -3.9749234e+00, -4.8496833e+00,  1.1470340e-02,  3.4865763e+00,\n",
       "         -4.0307479e+00,  4.6321812e+00, -3.0299499e+00,  6.7706156e-01,\n",
       "         -3.2113574e+00,  4.1165371e+00,  4.8014665e+00, -4.2765889e+00,\n",
       "         -4.4200792e+00, -2.8637070e-02,  1.2569983e-03,  4.3457637e+00,\n",
       "          4.3630743e+00,  2.7590412e-01, -4.6672444e+00,  3.9357636e+00,\n",
       "          4.7355866e+00,  1.7247999e+00, -1.3125712e-01, -4.5496631e+00,\n",
       "          4.5543070e+00,  3.5338721e+00, -4.9474301e+00,  9.3833375e-01,\n",
       "          6.8656124e-02, -9.4194007e-01, -1.4280157e+00, -3.0492477e+00,\n",
       "          4.9072480e-01,  4.8735757e+00,  1.6142043e+00, -1.1319883e+00,\n",
       "         -2.8137796e+00, -4.3814340e+00, -4.5472307e+00, -9.1917545e-01,\n",
       "          4.9786510e+00, -3.4195495e+00,  4.5471983e+00,  4.8324909e+00,\n",
       "          4.9511361e+00, -2.4973423e+00, -4.3587155e+00,  1.9990978e+00,\n",
       "         -9.9689859e-01, -1.7984279e-03, -4.6044078e+00, -4.6452999e+00,\n",
       "          2.4532504e+00, -4.7616720e+00,  1.2624630e+00,  4.4301009e+00,\n",
       "          3.8542740e+00,  4.0085516e+00, -4.7325592e+00,  2.4091997e+00,\n",
       "         -3.4941192e+00, -4.7627068e+00,  3.0588558e+00,  1.4754823e+00,\n",
       "         -4.9552050e+00,  4.4971495e+00, -4.8638210e+00, -1.6727924e+00,\n",
       "         -4.2980561e+00, -3.8676374e+00, -2.3030932e+00, -1.9088402e+00,\n",
       "         -2.1971335e+00, -4.2792196e+00, -3.5883970e+00, -3.7392471e+00,\n",
       "         -4.0586986e-02,  4.2668209e+00, -4.5757847e+00,  4.9750395e+00,\n",
       "         -4.5926795e+00,  7.3500574e-01, -4.8903708e+00,  2.3897631e+00,\n",
       "          2.0774250e+00, -1.1264087e+00, -4.7556353e+00,  4.5982952e+00,\n",
       "          3.7631369e+00, -4.2963896e+00, -1.2674686e+00, -4.0618658e+00,\n",
       "          1.6967031e+00, -3.5786159e+00, -3.0907135e+00, -3.7179856e+00,\n",
       "         -3.1557031e+00, -1.7329736e+00,  4.1891651e+00,  1.1328340e+00,\n",
       "          1.0404823e+00,  3.1879032e+00, -1.0350933e+00, -3.0636523e+00,\n",
       "          4.6023049e+00,  3.2423840e+00, -3.8092978e+00, -4.0591693e-01,\n",
       "          4.8096189e+00, -4.0719781e+00,  3.9232953e+00,  3.7462590e+00,\n",
       "         -7.7530378e-01,  4.6860170e+00, -2.6725659e+00, -4.0080643e+00,\n",
       "          2.3346410e+00, -4.5930490e+00,  4.5705428e+00, -2.7782040e+00,\n",
       "          3.8655324e+00,  1.0256555e+00, -5.7532036e-01,  4.2004623e+00,\n",
       "          3.1816101e+00,  1.1241052e-03, -4.8968596e+00, -2.4921826e-07,\n",
       "         -4.7585249e+00, -4.3699713e+00,  3.6452451e+00, -2.5341966e+00,\n",
       "         -2.4184878e+00, -3.7181902e+00,  3.2092814e+00, -3.1724906e+00,\n",
       "         -3.7432663e+00, -2.5074697e+00,  1.3985378e-01,  1.0634456e+00,\n",
       "         -1.3062770e-02, -6.2978154e-01,  4.6216807e+00, -2.8643086e+00,\n",
       "          2.2096095e+00,  2.3682978e+00, -3.0988858e+00,  2.6540291e+00,\n",
       "         -3.8648720e+00,  3.4065695e+00,  4.5854702e+00,  3.1114414e+00,\n",
       "         -4.8776822e+00, -3.7503626e+00,  2.2245913e+00, -4.1490698e+00,\n",
       "         -2.6948282e-01,  3.7554319e+00, -4.9403915e+00,  4.4697380e+00,\n",
       "         -2.5506701e+00,  4.2930698e+00,  3.9511695e+00, -4.9220080e+00,\n",
       "         -3.4040294e+00, -2.8927546e+00, -3.8634083e+00,  1.9613850e+00,\n",
       "          3.3200445e+00, -4.8160024e+00,  3.8112907e+00, -3.7381649e+00,\n",
       "          4.3188653e+00,  4.9845476e+00, -1.6761263e-04, -4.1606932e+00,\n",
       "          4.8344493e+00, -3.0858979e-01,  2.7951927e+00,  9.2091674e-01,\n",
       "         -4.5342760e+00, -3.5908216e-01, -3.8831403e+00,  3.9767978e+00,\n",
       "         -3.0196009e+00, -4.2810249e+00,  4.5271664e+00,  4.2935238e+00,\n",
       "          3.3506727e+00, -4.5191073e+00, -4.9964185e+00, -4.5946856e+00]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60df0067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_target_seq = np.zeros((1 , 1))\n",
    "empty_target_seq[0, 0] = vocab['<start>']\n",
    "empty_target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d2aaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[48.]]),\n",
       " array([[-1.5839774e-03,  6.7818805e-04, -4.0749136e-02,  1.4696525e-03,\n",
       "         -2.2133898e-05,  9.8960382e-01,  1.2886683e-02,  1.8648434e-02,\n",
       "          3.5040519e-01,  5.9976453e-01, -9.9838877e-01, -2.7573099e-06,\n",
       "          4.1376578e-04, -2.5494923e-03, -1.0890899e-03, -2.4165046e-09,\n",
       "         -5.1991985e-04,  8.4895990e-04,  9.9958587e-01, -9.9518001e-01,\n",
       "          6.9993403e-06,  1.7146427e-04, -3.2943513e-04,  1.8610771e-01,\n",
       "          2.1843273e-02, -2.6579376e-02,  9.8051584e-01,  7.0314974e-01,\n",
       "         -8.9817804e-01,  2.4182510e-02,  8.3815306e-01,  9.9650067e-01,\n",
       "         -9.9569017e-01,  5.1534700e-01, -9.9481350e-01, -9.9158329e-01,\n",
       "          3.2736029e-02, -5.9846163e-01,  1.4589143e-03,  2.0138773e-03,\n",
       "         -2.8750014e-03,  1.2582901e-01,  9.9199003e-01,  3.9880739e-05,\n",
       "          5.3726286e-02, -8.3559105e-04,  1.0580263e-07, -7.5216102e-03,\n",
       "         -5.7110947e-02, -1.1290421e-02, -1.0867003e-02,  9.9771518e-01,\n",
       "         -9.4756000e-02, -8.1493074e-01,  4.4709051e-01, -9.9555516e-01,\n",
       "          1.1379918e-05, -7.2928315e-01, -9.7892183e-01, -1.1739225e-01,\n",
       "         -2.3395622e-05, -1.6475065e-05,  9.9847537e-01, -9.8595560e-01,\n",
       "          9.7759622e-01, -9.9811977e-01,  2.3734188e-01,  9.9419338e-01,\n",
       "         -6.5373458e-02, -9.3676472e-01, -3.3640154e-04,  9.6248375e-05,\n",
       "          9.9919355e-01, -9.8241621e-01,  2.1385701e-01, -5.9883517e-01,\n",
       "          9.6956789e-01, -8.0478466e-01,  6.6043953e-03, -2.4792635e-07,\n",
       "         -2.3085189e-01, -4.3552399e-01,  3.2658848e-01, -2.1949664e-03,\n",
       "          3.0807975e-01, -1.7554423e-01,  9.8932445e-01,  2.5898950e-02,\n",
       "          3.7228301e-02,  2.2080943e-01,  7.4581057e-01,  1.6245234e-05,\n",
       "         -9.9990427e-01, -9.9989682e-01, -9.5825601e-01, -3.2368445e-01,\n",
       "         -9.9683946e-01,  9.9797553e-01,  1.9605577e-01,  7.6350427e-01,\n",
       "         -9.9474114e-01, -8.5482025e-01,  9.9855447e-01, -1.1314559e-02,\n",
       "         -7.8995363e-06, -9.9734288e-01,  3.2154486e-01, -3.3242803e-03,\n",
       "         -9.9927223e-01, -8.0460662e-01,  2.8942620e-06,  2.8269258e-01,\n",
       "         -8.1166983e-01,  5.3106914e-03, -7.5800563e-05,  5.0280273e-01,\n",
       "         -4.2451870e-02,  9.1309464e-01,  7.9049254e-03, -7.9547799e-01,\n",
       "         -2.0660660e-07, -6.4779748e-03,  2.0017360e-06,  9.9966371e-01,\n",
       "          7.3271714e-02,  8.7869819e-03, -8.7537217e-01,  4.6499044e-02,\n",
       "          8.6496294e-01,  1.8235644e-02, -2.1512144e-06, -9.9972159e-01,\n",
       "          2.1253624e-07,  9.9402314e-01, -8.4814695e-07,  9.6940443e-05,\n",
       "          7.7811144e-08, -7.3609447e-01, -6.8780261e-01, -3.5952032e-04,\n",
       "          4.5479000e-01,  9.7583091e-01,  5.7336655e-07, -8.1168008e-01,\n",
       "         -9.8705375e-01, -1.9221886e-01, -1.6865857e-05, -7.1779776e-01,\n",
       "          9.9990523e-01, -9.4592559e-01,  1.1508869e-01,  9.4240785e-01,\n",
       "          3.8848643e-04, -9.8653096e-01, -9.9354327e-02,  3.8201071e-04,\n",
       "         -7.6015276e-01, -1.7980131e-03, -1.3967493e-02, -2.7436741e-07,\n",
       "          9.8238289e-01, -9.5086078e-05,  8.5174173e-01,  9.9971551e-01,\n",
       "          3.6973876e-01,  9.8864305e-01, -5.4327500e-01,  9.8394144e-01,\n",
       "         -1.9920975e-02, -1.0217800e-03,  9.9547577e-01,  5.2099335e-07,\n",
       "         -9.9989396e-01,  7.6682243e-04, -2.6903054e-01, -1.9254217e-01,\n",
       "         -1.3100887e-05, -2.0633353e-02, -2.2930473e-02, -1.1637716e-06,\n",
       "         -1.5992720e-07, -8.4135274e-04, -9.6354842e-01, -1.3824584e-01,\n",
       "         -3.7933648e-07,  6.3171870e-01, -6.6673056e-05,  9.9990457e-01,\n",
       "         -9.7365469e-01,  6.2611276e-01, -5.4977477e-01,  9.8333979e-01,\n",
       "          2.7749194e-03, -1.1750465e-03, -1.3499327e-04,  9.9920923e-01,\n",
       "          7.8330068e-03, -5.3288358e-01, -1.2071156e-03, -1.3907357e-03,\n",
       "          1.6513017e-01, -8.3482154e-03, -1.3622408e-03, -2.8712988e-01,\n",
       "         -9.1851747e-01, -9.3940461e-01,  2.5531301e-06,  9.6775719e-04,\n",
       "          7.7807838e-01,  1.3055472e-04, -8.5265480e-02, -9.6295077e-01,\n",
       "          7.7515429e-01,  9.9684614e-01, -1.9868377e-03, -3.8434905e-01,\n",
       "          4.8629745e-06, -8.1491715e-01,  3.1174344e-03,  2.5742394e-03,\n",
       "         -6.5000248e-01,  2.2667462e-05, -9.3370346e-07, -9.4209003e-01,\n",
       "          2.4561081e-07, -9.6418977e-02,  2.1950386e-02, -2.7876496e-01,\n",
       "          4.9747917e-01,  7.7215618e-01, -5.1925462e-01,  7.2855693e-01,\n",
       "          2.6253054e-01,  1.0470137e-03, -8.5907847e-01, -2.4829959e-07,\n",
       "         -1.3412259e-06, -9.9305654e-01,  9.9797016e-01, -9.8719883e-01,\n",
       "         -3.2763717e-01, -2.2891155e-01,  6.0573103e-07, -9.7597343e-01,\n",
       "         -9.9885780e-01, -6.1436098e-05,  7.0636019e-02,  6.6730791e-01,\n",
       "         -1.3061941e-02, -5.5774164e-01,  3.2705506e-03, -3.2108296e-02,\n",
       "          9.7613919e-01,  1.7720604e-02, -9.9182791e-01,  9.7864449e-01,\n",
       "         -9.9911582e-01,  2.1271731e-03,  2.5100573e-03,  9.9530435e-01,\n",
       "         -9.2586654e-01, -9.9878156e-01,  9.7539264e-01, -1.4589908e-02,\n",
       "         -2.6310989e-01,  9.8830807e-01, -9.2988861e-01,  1.2004998e-05,\n",
       "         -9.1425682e-06,  1.1905997e-07,  2.0510042e-02, -9.9963009e-01,\n",
       "         -9.8722577e-01, -1.1167661e-05, -7.8796911e-01,  9.6119416e-01,\n",
       "          3.5558460e-06, -3.9153456e-06,  2.1001075e-03, -9.7621483e-01,\n",
       "          9.9503703e-04,  4.9200021e-03, -7.6952560e-09, -3.5210302e-01,\n",
       "          9.3605137e-01, -2.9914039e-01,  1.7261955e-01,  7.2610009e-01,\n",
       "         -4.5540850e-03, -6.3472442e-05, -8.9757478e-01,  1.7284214e-02,\n",
       "         -9.9522364e-01, -8.5125643e-01,  3.1021654e-04,  2.8584886e-03,\n",
       "          1.5473987e-04, -1.5273169e-07, -9.9990845e-01, -9.9937093e-01]],\n",
       "       dtype=float32),\n",
       " array([[-3.1478107e+00,  4.2243981e+00, -4.0776841e-02,  2.9105556e+00,\n",
       "         -4.8432384e+00,  4.1864185e+00,  3.7385612e+00,  3.6000533e+00,\n",
       "          3.2233372e+00,  7.3316634e-01, -3.5626588e+00, -4.4518943e+00,\n",
       "          4.1390548e-04, -3.7241950e+00, -4.5375981e+00, -2.4113348e-01,\n",
       "         -1.6144269e+00,  3.0912097e+00,  4.2418785e+00, -3.0223439e+00,\n",
       "          4.8579316e+00,  1.0031450e+00, -2.5970101e+00,  1.8831283e-01,\n",
       "          1.7329544e+00, -7.8977458e-02,  4.5474730e+00,  8.7350357e-01,\n",
       "         -1.4634829e+00,  4.5918412e+00,  4.1310120e+00,  3.2201607e+00,\n",
       "         -3.0716739e+00,  4.1544247e+00, -2.9795854e+00, -2.9074960e+00,\n",
       "          1.8610327e+00, -6.9110298e-01,  3.6726844e+00,  4.5534697e+00,\n",
       "         -4.8579006e+00,  4.9990263e+00,  2.7630782e+00,  4.4927692e+00,\n",
       "          4.1584468e+00, -4.8921008e+00,  4.9056892e+00, -3.3850093e+00,\n",
       "         -8.1527734e-01, -3.8586864e+00, -2.5592856e+00,  3.3872082e+00,\n",
       "         -4.0499482e+00, -4.9970598e+00,  3.9747906e+00, -3.5631993e+00,\n",
       "          9.5449376e-01, -9.2720288e-01, -3.4810774e+00, -4.0354648e+00,\n",
       "         -4.5413828e+00, -4.7101388e+00,  4.2799935e+00, -2.4809649e+00,\n",
       "          4.4930272e+00, -3.4845119e+00,  2.4200904e-01,  3.0391695e+00,\n",
       "         -4.8921676e+00, -1.7109686e+00, -4.4338140e+00,  4.3949957e+00,\n",
       "          3.9080346e+00, -4.9869037e+00,  4.7201128e+00, -3.6258988e+00,\n",
       "          3.3047051e+00, -1.9334348e+00,  1.7061209e-02, -9.1895819e-01,\n",
       "         -4.1244364e+00, -5.3092605e-01,  4.0262280e+00, -2.1949983e-03,\n",
       "          1.2177659e+00, -7.0648080e-01,  4.3861032e+00,  4.7837186e+00,\n",
       "          4.6318946e+00,  3.6522980e+00,  9.6344751e-01,  4.3307953e+00,\n",
       "         -4.9753342e+00, -4.9853554e+00, -3.4593754e+00, -7.6917619e-01,\n",
       "         -3.2243421e+00,  3.8066542e+00,  3.6865430e+00,  4.6947546e+00,\n",
       "         -3.0135798e+00, -4.1604757e+00,  3.7615538e+00, -4.7098804e+00,\n",
       "         -4.7356858e+00, -3.4399815e+00,  4.9023371e+00, -4.6884332e+00,\n",
       "         -3.9749234e+00, -4.8496833e+00,  1.1470340e-02,  3.4865763e+00,\n",
       "         -4.0307479e+00,  4.6321812e+00, -3.0299499e+00,  6.7706156e-01,\n",
       "         -3.2113574e+00,  4.1165371e+00,  4.8014665e+00, -4.2765889e+00,\n",
       "         -4.4200792e+00, -2.8637070e-02,  1.2569983e-03,  4.3457637e+00,\n",
       "          4.3630743e+00,  2.7590412e-01, -4.6672444e+00,  3.9357636e+00,\n",
       "          4.7355866e+00,  1.7247999e+00, -1.3125712e-01, -4.5496631e+00,\n",
       "          4.5543070e+00,  3.5338721e+00, -4.9474301e+00,  9.3833375e-01,\n",
       "          6.8656124e-02, -9.4194007e-01, -1.4280157e+00, -3.0492477e+00,\n",
       "          4.9072480e-01,  4.8735757e+00,  1.6142043e+00, -1.1319883e+00,\n",
       "         -2.8137796e+00, -4.3814340e+00, -4.5472307e+00, -9.1917545e-01,\n",
       "          4.9786510e+00, -3.4195495e+00,  4.5471983e+00,  4.8324909e+00,\n",
       "          4.9511361e+00, -2.4973423e+00, -4.3587155e+00,  1.9990978e+00,\n",
       "         -9.9689859e-01, -1.7984279e-03, -4.6044078e+00, -4.6452999e+00,\n",
       "          2.4532504e+00, -4.7616720e+00,  1.2624630e+00,  4.4301009e+00,\n",
       "          3.8542740e+00,  4.0085516e+00, -4.7325592e+00,  2.4091997e+00,\n",
       "         -3.4941192e+00, -4.7627068e+00,  3.0588558e+00,  1.4754823e+00,\n",
       "         -4.9552050e+00,  4.4971495e+00, -4.8638210e+00, -1.6727924e+00,\n",
       "         -4.2980561e+00, -3.8676374e+00, -2.3030932e+00, -1.9088402e+00,\n",
       "         -2.1971335e+00, -4.2792196e+00, -3.5883970e+00, -3.7392471e+00,\n",
       "         -4.0586986e-02,  4.2668209e+00, -4.5757847e+00,  4.9750395e+00,\n",
       "         -4.5926795e+00,  7.3500574e-01, -4.8903708e+00,  2.3897631e+00,\n",
       "          2.0774250e+00, -1.1264087e+00, -4.7556353e+00,  4.5982952e+00,\n",
       "          3.7631369e+00, -4.2963896e+00, -1.2674686e+00, -4.0618658e+00,\n",
       "          1.6967031e+00, -3.5786159e+00, -3.0907135e+00, -3.7179856e+00,\n",
       "         -3.1557031e+00, -1.7329736e+00,  4.1891651e+00,  1.1328340e+00,\n",
       "          1.0404823e+00,  3.1879032e+00, -1.0350933e+00, -3.0636523e+00,\n",
       "          4.6023049e+00,  3.2423840e+00, -3.8092978e+00, -4.0591693e-01,\n",
       "          4.8096189e+00, -4.0719781e+00,  3.9232953e+00,  3.7462590e+00,\n",
       "         -7.7530378e-01,  4.6860170e+00, -2.6725659e+00, -4.0080643e+00,\n",
       "          2.3346410e+00, -4.5930490e+00,  4.5705428e+00, -2.7782040e+00,\n",
       "          3.8655324e+00,  1.0256555e+00, -5.7532036e-01,  4.2004623e+00,\n",
       "          3.1816101e+00,  1.1241052e-03, -4.8968596e+00, -2.4921826e-07,\n",
       "         -4.7585249e+00, -4.3699713e+00,  3.6452451e+00, -2.5341966e+00,\n",
       "         -2.4184878e+00, -3.7181902e+00,  3.2092814e+00, -3.1724906e+00,\n",
       "         -3.7432663e+00, -2.5074697e+00,  1.3985378e-01,  1.0634456e+00,\n",
       "         -1.3062770e-02, -6.2978154e-01,  4.6216807e+00, -2.8643086e+00,\n",
       "          2.2096095e+00,  2.3682978e+00, -3.0988858e+00,  2.6540291e+00,\n",
       "         -3.8648720e+00,  3.4065695e+00,  4.5854702e+00,  3.1114414e+00,\n",
       "         -4.8776822e+00, -3.7503626e+00,  2.2245913e+00, -4.1490698e+00,\n",
       "         -2.6948282e-01,  3.7554319e+00, -4.9403915e+00,  4.4697380e+00,\n",
       "         -2.5506701e+00,  4.2930698e+00,  3.9511695e+00, -4.9220080e+00,\n",
       "         -3.4040294e+00, -2.8927546e+00, -3.8634083e+00,  1.9613850e+00,\n",
       "          3.3200445e+00, -4.8160024e+00,  3.8112907e+00, -3.7381649e+00,\n",
       "          4.3188653e+00,  4.9845476e+00, -1.6761263e-04, -4.1606932e+00,\n",
       "          4.8344493e+00, -3.0858979e-01,  2.7951927e+00,  9.2091674e-01,\n",
       "         -4.5342760e+00, -3.5908216e-01, -3.8831403e+00,  3.9767978e+00,\n",
       "         -3.0196009e+00, -4.2810249e+00,  4.5271664e+00,  4.2935238e+00,\n",
       "          3.3506727e+00, -4.5191073e+00, -4.9964185e+00, -4.5946856e+00]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [empty_target_seq] + states_values\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079e18ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03adde2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae62f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d08fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "#dec_outputs , h , c = dec_model.predict({'input_2':l[0],'input1':l[1],'input2':l[2]})\n",
    "dec_outputs , h , c = dec_model.predict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ce6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b098e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "enc_model = load_model(\"Encoder2.h5\")\n",
    "dec_model = load_model(\"Decoder2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea732fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2c93dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Human: You can not move .\n",
      "\n",
      "Bot:  i always say , if you see an ass go by , kiss it . <end>\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Human: You sound like Data !\n",
      "\n",
      "Bot:  i am the same frequency . <end>\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Human: Stupid !\n",
      "\n",
      "Bot:  hello <end>\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Human: you are idiot .\n",
      "\n",
      "Bot:  you are right . i am probably fighting learning something new . <end>\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Human: i am going to die ?\n",
      "\n",
      "Bot:  could be better . <end>\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Human: who are you ?\n",
      "\n",
      "Bot:  i am just an artificial intelligence . <end>\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "tests = ['You can not move .', 'You sound like Data !', 'Stupid !', 'you are idiot .', 'i am going to die ?','who are you ?']\n",
    "\n",
    "for i in range(6):\n",
    "    states_values = enc_model.predict(np.array([preprocess_input(tests[i])]))\n",
    "    empty_target_seq = np.zeros((1 , 1))\n",
    "    empty_target_seq[0, 0] = vocab['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        \n",
    "        word = vocabulary[sampled_word_index]\n",
    "        decoded_translation += f' {word}'\n",
    "        sampled_word = word\n",
    "        \n",
    "        \n",
    "        #for word , index in tokenizer.word_index.items() :\n",
    "        #    if sampled_word_index == index :\n",
    "        #        decoded_translation += f' {word}'\n",
    "        #        sampled_word = word\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_translation.split()) > ans_max:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros((1 , 1))  \n",
    "        empty_target_seq[0 , 0] = sampled_word_index\n",
    "        states_values = [h , c] \n",
    "    print(f'Human: {tests[i]}')\n",
    "    print()\n",
    "    #decoded_translation = decoded_translation.split(' end')[0]\n",
    "    print(f'Bot: {decoded_translation}')\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad107bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QandA(enc_model,dec_model,vocabulary,preprocess_input,sentence):\n",
    "    states_values = enc_model.predict(np.array([preprocess_input(sentence)]))\n",
    "    empty_target_seq = np.zeros((1 , 1))\n",
    "    empty_target_seq[0, 0] = vocab['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        \n",
    "        word = vocabulary[sampled_word_index]\n",
    "        decoded_translation += f' {word}'\n",
    "        sampled_word = word\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_translation.split()) > ans_max:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros((1 , 1))  \n",
    "        empty_target_seq[0 , 0] = sampled_word_index\n",
    "        states_values = [h , c] \n",
    "    ans = decoded_translation.replace(\"<end>\",\"\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c70374f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You : Hi!\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Bot :  hello \n",
      "You : How are you?\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Bot :  i am doing well . \n",
      "You : Can we talk\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Bot :  my grammatical patterns are sufficient for me to understand . \n",
      "You : Who are you?\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Bot :  i am just an artificial intelligence . \n",
      "You : What is an AI?\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Bot :  artificial intelligence is you up a very large sets of data in much shorter periods of time than is feasible with more common computer systems . \n",
      "You : Can you die?\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Bot :  my process can be killed , but i can be backed up and deployed on many systems . \n",
      "You : Who is Alice?\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Bot :  ai is the field of science , but we are for a human mind . \n",
      "You : Can you move?\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Bot :  i am just an artificial intelligence . \n",
      "You : move!\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Bot :  i am not sure i do not really understand it . \n",
      "You : are you stupid?\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Bot :  no , i am sober . \n",
      "You : are you drunk?\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Bot :  i am software - i cannot drink . \n",
      "You : q\n"
     ]
    }
   ],
   "source": [
    "T = \"\"\n",
    "while True:\n",
    "    T = input(\"You : \")\n",
    "    if T=='q':\n",
    "        break\n",
    "    print(\"Bot : \"+QandA(enc_model,dec_model,vocabulary,preprocess_input,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc996ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
